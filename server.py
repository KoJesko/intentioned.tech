#!/usr/bin/env python3
# The next 112 lines are auto-generated by AI to ensure environment setup
"""Speech + LLM server with automatic environment bootstrapping."""

from __future__ import annotations

import gc
import os
import subprocess
import sys
from pathlib import Path

# --- Ensure we are running inside the project virtualenv (myenv) ---
PROJECT_ROOT = Path(__file__).resolve().parent.resolve()
VENV_DIR = PROJECT_ROOT / "myenv"
VENV_BIN = "Scripts" if os.name == "nt" else "bin"
VENV_PYTHON = VENV_DIR / VENV_BIN / ("python.exe" if os.name == "nt" else "python")

if not VENV_DIR.exists():
    print(f"üîß Creating project virtualenv at {VENV_DIR}...")
    subprocess.check_call([sys.executable, "-m", "venv", str(VENV_DIR)])

if Path(sys.executable).resolve() != VENV_PYTHON.resolve():
    if not VENV_PYTHON.exists():
        raise RuntimeError(
            f"Expected virtualenv interpreter at {VENV_PYTHON}, but it was not found."
        )
    print(f"‚Üª Re-launching inside project venv: {VENV_DIR}")
    os.execv(str(VENV_PYTHON), [str(VENV_PYTHON), *sys.argv])


# --- Ensure ffmpeg is installed (required for audio processing) ---
def _check_ffmpeg():
    """Check if ffmpeg is available, install if missing on Windows."""
    import shutil

    if shutil.which("ffmpeg"):
        return True

    if os.name == "nt":
        print("üîß ffmpeg not found. Installing via winget...")
        try:
            subprocess.run(
                [
                    "winget",
                    "install",
                    "--id",
                    "BtbN.FFmpeg.GPL",
                    "-e",
                    "--accept-package-agreements",
                    "--accept-source-agreements",
                ],
                check=True,
            )
            print(
                "‚úÖ ffmpeg installed. Please restart the server for PATH changes to take effect."
            )
            sys.exit(0)
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è Failed to install ffmpeg via winget. Please install manually:")
            print("   winget install --id BtbN.FFmpeg.GPL")
            return False
        except FileNotFoundError:
            print("‚ö†Ô∏è winget not found. Please install ffmpeg manually:")
            print("   https://ffmpeg.org/download.html")
            return False
    else:
        print("‚ö†Ô∏è ffmpeg not found. Please install it:")
        print("   Ubuntu/Debian: sudo apt install ffmpeg")
        print("   macOS: brew install ffmpeg")
        return False
    return True


_check_ffmpeg()


import asyncio
import base64
import io
import json
import re
import tempfile
import wave
import ast
from collections import Counter
from difflib import SequenceMatcher
from threading import Thread, Lock
from typing import Optional

# --- Violation Tracking for Repeated Offense Detection ---
# Stores recent violations per session to detect repeated similar violations
# This system was AI generated but human-reviewed and adjusted for accuracy and fine tuning. The thresholds were set based on typical user behavior patterns. The sensitivity is very high at this stage and requires adjustments.
_violation_tracker: dict[str, list[dict]] = {}
_violation_tracker_lock = Lock()
VIOLATION_REPORT_THRESHOLD = 3  # Number of unique violations before reporting to host
VIOLATION_STOP_THRESHOLD = 5  # Number of unique violations before stopping conversation
SEVERE_VIOLATION_STOP_THRESHOLD = (
    2  # Number of severe violations before immediate shutdown
)

import edge_tts
import numpy as np
import pyttsx3
import soundfile as sf
import torch
from fastapi import (
    FastAPI,
    WebSocket,
    WebSocketDisconnect,
    HTTPException,
    UploadFile,
    File,
)
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# Vosk for non-AI speech recognition (optional, falls back to Whisper if not available)
try:
    from vosk import Model as VoskModel, KaldiRecognizer

    VOSK_AVAILABLE = True
except ImportError:
    VOSK_AVAILABLE = False
    print("‚ö†Ô∏è Vosk not installed. Will use Whisper for STT.")

from transformers.models.auto.modeling_auto import AutoModelForCausalLM
from transformers.models.auto.tokenization_auto import AutoTokenizer
from transformers.generation.streamers import TextIteratorStreamer
from transformers.pipelines import pipeline

# Initialize the App
app = FastAPI()


# --- Runtime Configuration (changeable via UI) ---
# Emerging technology, LLM coded due to being out of scope
# These settings can be modified at runtime without restarting the server
class RuntimeConfig:
    """Holds runtime-configurable settings that can be changed via the web UI."""

    def __init__(self):
        # TTS Settings
        self.tts_engine = os.getenv(
            "TTS_ENGINE", "vibevoice"
        )  # vibevoice, edge, pyttsx3
        self.vibevoice_voice = os.getenv("VIBEVOICE_VOICE", "en-Grace_woman")
        self.edge_tts_voice = os.getenv("EDGE_TTS_VOICE", "female_us")

        # STT Settings (not Whisper)
        self.stt_engine = os.getenv("STT_ENGINE", "wav2vec2")  # wav2vec2, vosk

        # LLM Settings
        self.llm_max_tokens = int(os.getenv("LLM_MAX_NEW_TOKENS", "150"))
        self.llm_temperature = float(os.getenv("LLM_TEMPERATURE", "0.7"))

    def to_dict(self):
        return {
            "tts_engine": self.tts_engine,
            "vibevoice_voice": self.vibevoice_voice,
            "edge_tts_voice": self.edge_tts_voice,
            "stt_engine": self.stt_engine,
            "llm_max_tokens": self.llm_max_tokens,
            "llm_temperature": self.llm_temperature,
        }

    def update(self, settings: dict):
        if "tts_engine" in settings and settings["tts_engine"] in [
            "vibevoice",
            "edge",
            "pyttsx3",
        ]:
            self.tts_engine = settings["tts_engine"]
        if "vibevoice_voice" in settings:
            self.vibevoice_voice = settings["vibevoice_voice"]
        if "edge_tts_voice" in settings:
            self.edge_tts_voice = settings["edge_tts_voice"]
        if "stt_engine" in settings and settings["stt_engine"] in ["wav2vec2", "vosk"]:
            self.stt_engine = settings["stt_engine"]
        if "llm_max_tokens" in settings:
            self.llm_max_tokens = max(50, min(500, int(settings["llm_max_tokens"])))
        if "llm_temperature" in settings:
            self.llm_temperature = max(
                0.1, min(2.0, float(settings["llm_temperature"]))
            )


# Global runtime config instance
runtime_config = RuntimeConfig()

# Available voice options for the UI, boilerplate filled in by AI
VIBEVOICE_VOICES = [
    {"id": "en-Carter_man", "name": "Carter (Male)"},
    {"id": "en-Davis_man", "name": "Davis (Male)"},
    {"id": "en-Emma_woman", "name": "Emma (Female)"},
    {"id": "en-Frank_man", "name": "Frank (Male)"},
    {"id": "en-Grace_woman", "name": "Grace (Female)"},
    {"id": "en-Mike_man", "name": "Mike (Male)"},
]

# Available Edge TTS voices for the UI, boilerplate filled in by AI
EDGE_TTS_VOICES = [
    {"id": "female_us", "name": "Aria (Female, US)"},
    {"id": "male_us", "name": "Guy (Male, US)"},
    {"id": "female_uk", "name": "Sonia (Female, UK)"},
    {"id": "male_uk", "name": "Ryan (Male, UK)"},
    {"id": "female_au", "name": "Natasha (Female, AU)"},
    {"id": "male_au", "name": "William (Male, AU)"},
]


# --- Settings API Endpoints --- Human Coded
@app.get("/api/settings")
async def get_settings():
    """Get current runtime settings and available options."""
    return {
        "current": runtime_config.to_dict(),
        "options": {
            "tts_engines": [
                {"id": "vibevoice", "name": "VibeVoice (Natural AI Voice)"},
                {"id": "edge", "name": "Edge TTS (Microsoft Online)"},
                {"id": "pyttsx3", "name": "pyttsx3 (Offline)"},
            ],
            "vibevoice_voices": VIBEVOICE_VOICES,
            "edge_tts_voices": EDGE_TTS_VOICES,
            "stt_engines": [
                {"id": "wav2vec2", "name": "Wav2Vec2 (AI, No Hallucinations)"},
                {"id": "vosk", "name": "Vosk (Traditional, Fast)"},
            ],
        },
    }


@app.post("/api/settings")
async def update_settings(settings: dict):
    """Update runtime settings."""
    runtime_config.update(settings)
    return {"status": "ok", "current": runtime_config.to_dict()}


# --- Session Results Storage (CSV format) ---
RESULTS_DIR = PROJECT_ROOT / "session_results"
RESULTS_DIR.mkdir(exist_ok=True)

# CSV columns for session results
CSV_COLUMNS = [
    "timestamp",
    "session_id",
    "scenario",
    "overall_score",
    "filler_word_count",
    "filler_word_score",
    "filler_details",
    "delivery_score",
    "delivery_feedback",
    "tone_score",
    "tone_feedback",
    "microaggressions_detected",
    "microaggressions_feedback",
    "eye_contact_score",
    "eye_contact_percentage",
    "eye_contact_feedback",
    "speech_pacing_score",
    "speech_pacing_avg_gap",
    "speech_pacing_feedback",
    "speaking_pace_score",
    "speaking_pace_wpm",
    "speaking_pace_feedback",
    "response_time_score",
    "response_time_avg",
    "response_time_feedback",
    "interruptions_score",
    "interruptions_count",
    "interruptions_feedback",
    "ai_summary",
]


# AI Generated CSV Maker
def _flatten_result_to_row(result: dict, session_id: str | None = None) -> dict:
    """Flatten a nested result dict into a CSV row."""
    from datetime import datetime

    row = {
        "timestamp": datetime.now().isoformat(),
        "session_id": session_id or "",
        "scenario": result.get("scenario", "general"),
        "overall_score": result.get("overall_score", 0),
        # Filler words
        "filler_word_count": result.get("filler_words", {}).get("count", 0),
        "filler_word_score": result.get("filler_words", {}).get("score", ""),
        "filler_details": str(result.get("filler_words", {}).get("details", {})),
        # Delivery
        "delivery_score": result.get("delivery", {}).get("score", ""),
        "delivery_feedback": result.get("delivery", {}).get("feedback", ""),
        # Tone
        "tone_score": result.get("tone", {}).get("score", ""),
        "tone_feedback": result.get("tone", {}).get("feedback", ""),
        # Microaggressions
        "microaggressions_detected": result.get("microaggressions", {}).get(
            "detected", False
        ),
        "microaggressions_feedback": result.get("microaggressions", {}).get(
            "feedback", ""
        ),
        # Eye contact
        "eye_contact_score": result.get("eye_contact", {}).get("score", "")
        if result.get("eye_contact")
        else "",
        "eye_contact_percentage": result.get("eye_contact", {}).get("percentage", 0)
        if result.get("eye_contact")
        else "",
        "eye_contact_feedback": result.get("eye_contact", {}).get("feedback", "")
        if result.get("eye_contact")
        else "",
        # Speech pacing
        "speech_pacing_score": result.get("speech_pacing", {}).get("score", "")
        if result.get("speech_pacing")
        else "",
        "speech_pacing_avg_gap": result.get("speech_pacing", {}).get("avg_gap", 0)
        if result.get("speech_pacing")
        else "",
        "speech_pacing_feedback": result.get("speech_pacing", {}).get("feedback", "")
        if result.get("speech_pacing")
        else "",
        # Speaking pace (WPM)
        "speaking_pace_score": result.get("speaking_pace", {}).get("score", "")
        if result.get("speaking_pace")
        else "",
        "speaking_pace_wpm": result.get("speaking_pace", {}).get("avg_wpm", 0)
        if result.get("speaking_pace")
        else "",
        "speaking_pace_feedback": result.get("speaking_pace", {}).get("feedback", "")
        if result.get("speaking_pace")
        else "",
        # Response time
        "response_time_score": result.get("response_time", {}).get("score", "")
        if result.get("response_time")
        else "",
        "response_time_avg": result.get("response_time", {}).get("avg_time", 0)
        if result.get("response_time")
        else "",
        "response_time_feedback": result.get("response_time", {}).get("feedback", "")
        if result.get("response_time")
        else "",
        # Interruptions
        "interruptions_score": result.get("interruptions", {}).get("score", "")
        if result.get("interruptions")
        else "",
        "interruptions_count": result.get("interruptions", {}).get("count", 0)
        if result.get("interruptions")
        else "",
        "interruptions_feedback": result.get("interruptions", {}).get("feedback", "")
        if result.get("interruptions")
        else "",
        # AI Summary
        "ai_summary": result.get("ai_summary", ""),
    }
    return row


## AI generated CSV Loader, high amount of boilerplate
def _row_to_result(row: dict) -> dict:
    """Convert a CSV row back to a nested result dict."""
    # Safely parse filler_details - use ast.literal_eval instead of eval for security
    filler_details = {}
    if row.get("filler_details"):
        try:
            filler_details = ast.literal_eval(row.get("filler_details", "{}"))
        except (ValueError, SyntaxError):
            filler_details = {}

    result = {
        "scenario": row.get("scenario", "general"),
        "overall_score": int(row.get("overall_score", 0)),
        "filler_words": {
            "count": int(row.get("filler_word_count", 0)),
            "score": row.get("filler_word_score", ""),
            "details": filler_details,
        },
        "delivery": {
            "score": row.get("delivery_score", ""),
            "feedback": row.get("delivery_feedback", ""),
        },
        "tone": {
            "score": row.get("tone_score", ""),
            "feedback": row.get("tone_feedback", ""),
        },
        "microaggressions": {
            "detected": row.get("microaggressions_detected", "").lower() == "true"
            if isinstance(row.get("microaggressions_detected"), str)
            else bool(row.get("microaggressions_detected")),
            "feedback": row.get("microaggressions_feedback", ""),
        },
        "ai_summary": row.get("ai_summary", ""),
    }

    # Optional fields
    if row.get("eye_contact_score"):
        result["eye_contact"] = {
            "score": row.get("eye_contact_score", ""),
            "percentage": float(row.get("eye_contact_percentage", 0)),
            "feedback": row.get("eye_contact_feedback", ""),
        }
    if row.get("speech_pacing_score"):
        result["speech_pacing"] = {
            "score": row.get("speech_pacing_score", ""),
            "avg_gap": float(row.get("speech_pacing_avg_gap", 0)),
            "feedback": row.get("speech_pacing_feedback", ""),
        }
    if row.get("speaking_pace_score"):
        result["speaking_pace"] = {
            "score": row.get("speaking_pace_score", ""),
            "avg_wpm": float(row.get("speaking_pace_wpm", 0)),
            "feedback": row.get("speaking_pace_feedback", ""),
        }
    if row.get("response_time_score"):
        result["response_time"] = {
            "score": row.get("response_time_score", ""),
            "avg_time": float(row.get("response_time_avg", 0)),
            "feedback": row.get("response_time_feedback", ""),
        }
    if row.get("interruptions_score"):
        result["interruptions"] = {
            "score": row.get("interruptions_score", ""),
            "count": int(row.get("interruptions_count", 0)),
            "feedback": row.get("interruptions_feedback", ""),
        }

    return result


# AI Generated Session Results API
def save_session_result(result: dict, session_id: str | None = None) -> str:
    """Save a session analysis result to CSV file."""
    import csv
    from datetime import datetime

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if session_id:
        filename = f"session_{timestamp}_{session_id[:8]}.csv"
    else:
        filename = f"session_{timestamp}.csv"

    filepath = RESULTS_DIR / filename
    row = _flatten_result_to_row(result, session_id)

    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=CSV_COLUMNS)
        writer.writeheader()
        writer.writerow(row)

    print(f"[Results] Saved session result to {filename}")
    return filename


# Does exactly what it says in the def name, boilerplate, 25% AI generated
def list_session_results() -> list[dict]:
    """List all saved session results (CSV files)."""
    import csv

    results = []
    for filepath in sorted(RESULTS_DIR.glob("session_*.csv"), reverse=True):
        try:
            with open(filepath, "r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                row = next(reader, None)
                if row:
                    results.append(
                        {
                            "filename": filepath.name,
                            "timestamp": row.get("timestamp", "Unknown"),
                            "overall_score": int(row.get("overall_score", 0)),
                            "scenario": row.get("scenario", "general"),
                        }
                    )
        except Exception as e:
            print(f"[Results] Error reading {filepath.name}: {e}")
    return results


def load_session_result(filename: str) -> dict | None:
    """Load a specific session result from CSV."""
    import csv

    # Security: ensure filename is safe (no path traversal)
    # Filled in by AI to prevent common vulnerabilities
    # Utilized GitHub Security best practices for file handling
    safe_filename = Path(filename).name
    if not safe_filename.endswith(".csv") or not safe_filename.startswith("session_"):
        return None

    filepath = RESULTS_DIR / safe_filename
    if not filepath.exists():
        return None

    try:
        with open(filepath, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            row = next(reader, None)
            if row:
                return {
                    "timestamp": row.get("timestamp"),
                    "session_id": row.get("session_id"),
                    "result": _row_to_result(row),
                }
        return None
    except Exception as e:
        print(f"[Results] Error loading {filename}: {e}")
        return None


# AI Generated Delete Session Result - security conscious
def delete_session_result(filename: str) -> bool:
    """Delete a specific session result."""
    safe_filename = Path(filename).name
    if not safe_filename.endswith(".csv") or not safe_filename.startswith("session_"):
        return False

    filepath = RESULTS_DIR / safe_filename
    if filepath.exists():
        filepath.unlink()
        return True
    return False


# --- API Endpoints for Session Results Management ---
# AI Generated Code


# API Endpoints for Session Results
@app.get("/api/results")
async def get_results_list():
    """Get list of saved session results."""
    return {"results": list_session_results()}


# API Endpoint to get a specific session result
@app.get("/api/results/{filename}")
async def get_result(filename: str):
    """Get a specific session result."""
    result = load_session_result(filename)
    if result is None:
        return {"error": "Result not found"}
    return result


# API Endpoint to delete a specific session result
@app.delete("/api/results/{filename}")
async def delete_result(filename: str):
    """Delete a specific session result."""
    if delete_session_result(filename):
        return {"status": "ok"}
    return {"error": "Result not found"}


# API Endpoint to import a session result from CSV
@app.post("/api/results/import")
async def import_result(file: UploadFile = File(...)):
    """Import a session result from uploaded CSV file."""
    import csv
    from io import StringIO
    from datetime import datetime

    if not file.filename or not file.filename.endswith(".csv"):
        return {"error": "Invalid file format - must be a CSV file"}

    try:
        # Read the uploaded CSV content
        content = await file.read()
        content_str = content.decode("utf-8")
        reader = csv.DictReader(StringIO(content_str))
        row = next(reader, None)

        if row is None:
            return {"error": "CSV file is empty"}

        # Save as imported file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"session_{timestamp}_imported.csv"
        filepath = RESULTS_DIR / filename

        # Write the CSV with our standard columns
        with open(filepath, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=CSV_COLUMNS)
            writer.writeheader()
            # Fill in missing columns with empty strings
            clean_row = {col: row.get(col, "") for col in CSV_COLUMNS}
            writer.writerow(clean_row)

        return {"status": "ok", "filename": filename}
    except Exception as e:
        return {"error": f"Failed to import CSV: {str(e)}"}


# API Endpoint to export a specific session result as CSV
@app.get("/api/results/export/{filename}")
async def export_result(filename: str):
    """Export a session result as downloadable CSV."""
    from fastapi.responses import FileResponse

    safe_filename = Path(filename).name
    if not safe_filename.endswith(".csv") or not safe_filename.startswith("session_"):
        return {"error": "Invalid filename"}

    filepath = RESULTS_DIR / safe_filename
    if not filepath.exists():
        return {"error": "Result not found"}

    return FileResponse(
        path=filepath,
        media_type="text/csv",
        filename=safe_filename,
    )


# API Endpoint to export ALL session results as a single CSV
@app.get("/api/results/export-all")
async def export_all_results():
    """Export ALL session results as a single combined CSV file."""
    import csv
    from io import StringIO
    from fastapi.responses import Response
    from datetime import datetime

    # Collect all rows from all CSV files
    all_rows = []
    for filepath in sorted(RESULTS_DIR.glob("session_*.csv")):
        try:
            with open(filepath, "r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    all_rows.append(row)
        except Exception as e:
            print(f"[Results] Error reading {filepath.name}: {e}")

    # Create combined CSV
    output = StringIO()
    writer = csv.DictWriter(output, fieldnames=CSV_COLUMNS)
    writer.writeheader()
    for row in all_rows:
        # Ensure all columns exist
        clean_row = {col: row.get(col, "") for col in CSV_COLUMNS}
        writer.writerow(clean_row)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return Response(
        content=output.getvalue(),
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename=all_sessions_{timestamp}.csv"
        },
    )


# --- Analysis Request/Response Models ---
# The block below is human generated based on typical analysis needs
class AnalysisRequest(BaseModel):
    transcript: list[dict]  # List of {"role": "user"|"assistant", "content": str}
    eye_contact_data: Optional[list[dict]] = None  # Optional webcam data
    speech_timestamps: Optional[list[dict]] = (
        None  # Optional speech timing data {"start": float, "end": float, "word_count": int}
    )
    response_times: Optional[list[float]] = (
        None  # Time between AI done speaking and user starts speaking
    )
    interruption_count: int = 0  # Number of times user interrupted the AI
    scenario: str = "general"


# --- SoTA Analysis Response Model /s ---
# The block below is human generated based on typical analysis needs
class AnalysisResponse(BaseModel):
    filler_words: dict  # {"count": int, "target": int, "details": dict, "score": str}
    delivery: dict  # {"score": str, "feedback": str}
    tone: dict  # {"score": str, "feedback": str}
    microaggressions: dict  # {"detected": bool, "feedback": str}
    eye_contact: Optional[dict] = (
        None  # {"score": str, "percentage": float, "feedback": str}
    )
    speech_pacing: Optional[dict] = (
        None  # {"score": str, "avg_gap": float, "feedback": str}
    )
    speaking_pace: Optional[dict] = (
        None  # {"score": str, "wpm": float, "feedback": str}
    )
    response_time: Optional[dict] = (
        None  # {"score": str, "avg_time": float, "feedback": str}
    )
    interruptions: Optional[dict] = (
        None  # {"score": str, "count": int, "feedback": str}
    )
    overall_score: int = 0  # 0-100 overall score
    ai_summary: str = ""  # AI-generated summary


# --- Analysis Functions ---
# Human generated analysis functions below, AI bug fixed some code below
def score_to_points(score_str: str) -> int:
    """Convert emoji score string to numeric points."""
    if "Excellent" in score_str or "üü¢" in score_str:
        return 100
    elif "Good" in score_str or "üü°" in score_str:
        return 75
    elif "Needs Work" in score_str or "üü†" in score_str:
        return 50
    elif "Poor" in score_str or "üî¥" in score_str:
        return 25
    return 0


# AI Generated Overall Score Calculator, adjustable, human-reviewed, fine-tuned.
def calculate_overall_score(
    filler: dict,
    delivery: dict,
    tone: dict,
    microaggressions: dict,
    pacing: Optional[dict],
    eye_contact: Optional[dict],
    response_time: Optional[dict] = None,
) -> int:
    """Calculate overall score out of 100."""
    scores = []
    weights = []

    # Filler words (weight: 20%)
    scores.append(score_to_points(filler.get("score", "")))
    weights.append(0.20)

    # Delivery (weight: 25%)
    scores.append(score_to_points(delivery.get("score", "")))
    weights.append(0.25)

    # Tone (weight: 25%)
    scores.append(score_to_points(tone.get("score", "")))
    weights.append(0.25)

    # Microaggressions (weight: 15%) - no detected = 100, detected = 25
    if not microaggressions.get("detected", False):
        scores.append(100)
    else:
        scores.append(25)
    weights.append(0.15)

    # Pacing (weight: 8% if available)
    if pacing:
        scores.append(score_to_points(pacing.get("score", "")))
        weights.append(0.08)

    # Eye contact (weight: 5% if available)
    if eye_contact:
        scores.append(score_to_points(eye_contact.get("score", "")))
        weights.append(0.05)

    # Response time (weight: 7% if available)
    if response_time:
        scores.append(score_to_points(response_time.get("score", "")))
        weights.append(0.07)

    # Normalize weights
    total_weight = sum(weights)
    normalized_weights = [w / total_weight for w in weights]

    # Calculate weighted average
    overall = sum(s * w for s, w in zip(scores, normalized_weights))
    return round(overall)


# New summary function
# Hybrid generation and edited by AI and human
def generate_ai_summary(
    score: int,
    filler: dict,
    delivery: dict,
    tone: dict,
    microaggressions: dict,
    pacing: Optional[dict],
    eye_contact: Optional[dict],
    response_time: Optional[dict] = None,
) -> str:
    """Generate a unique AI summary of the performance using the LLM."""
    # Import model_manager from global scope (defined later in file)
    global model_manager

    # Build context for the LLM
    metrics_context = f"""Performance Metrics:
- Overall Score: {score}/100
- Filler Words: {filler.get("count", 0)} used (target: ‚â§5), rated {filler.get("score", "N/A")}
- Delivery: {delivery.get("score", "N/A")} - {delivery.get("feedback", "")}
- Tone: {tone.get("score", "N/A")} - {tone.get("feedback", "")}
- Microaggressions: {"Detected - " + microaggressions.get("feedback", "") if microaggressions.get("detected") else "None detected"}"""

    if pacing:
        metrics_context += f"\n- Speech Pacing: {pacing.get('score', 'N/A')} (avg gap: {pacing.get('avg_gap', 0):.1f}s)"
    if eye_contact:
        metrics_context += f"\n- Eye Contact: {eye_contact.get('score', 'N/A')} ({eye_contact.get('percentage', 0):.1f}%)"
    if response_time:
        metrics_context += f"\n- Response Time: {response_time.get('score', 'N/A')} (avg: {response_time.get('avg_time', 0):.1f}s)"

    summary_prompt = [
        {
            "role": "system",
            "content": """You are a supportive communication coach providing personalized feedback summaries.
Generate a brief, unique 2-4 sentence summary based on the performance metrics provided.
Be encouraging but honest. Highlight 1-2 strengths and 1-2 specific areas for improvement.
Vary your language and phrasing - never use the same generic phrases.
Keep the tone professional yet warm. Do not use bullet points or lists.""",
        },
        {
            "role": "user",
            "content": f"{metrics_context}\n\nProvide a personalized performance summary:",
        },
    ]

    # Try LLM-based summary generation
    try:
        # Access model_manager from global scope (defined later in file)
        _manager = globals().get("model_manager")
        if (
            _manager is not None
            and _manager.is_loaded
            and _manager.llm_model is not None
            and _manager.llm_tokenizer is not None
        ):
            tokenizer = _manager.llm_tokenizer  # Local reference for type checker
            input_ids = tokenizer.apply_chat_template(
                summary_prompt,
                return_tensors="pt",
                add_generation_prompt=True,
            ).to(torch.device)

            with torch.no_grad():
                outputs = _manager.llm_model.generate(
                    input_ids,
                    max_new_tokens=120,
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.9,
                    pad_token_id=tokenizer.pad_token_id,
                )

            summary = tokenizer.decode(
                outputs[0][input_ids.shape[1] :], skip_special_tokens=True
            ).strip()

            if summary and len(summary) > 20:
                return summary
    except Exception as e:
        print(f"‚ö†Ô∏è LLM summary generation failed: {e}")

    # Fallback to template-based summary if LLM fails
    return _generate_fallback_summary(
        score,
        filler,
        delivery,
        tone,
        microaggressions,
        pacing,
        eye_contact,
        response_time,
    )


def _generate_fallback_summary(
    score: int,
    filler: dict,
    delivery: dict,
    tone: dict,
    microaggressions: dict,
    pacing: Optional[dict],
    eye_contact: Optional[dict],
    response_time: Optional[dict] = None,
) -> str:
    """Fallback template-based summary when LLM is unavailable."""
    summaries = []

    if score >= 90:
        summaries.append("Excellent communication skills demonstrated.")
    elif score >= 75:
        summaries.append("Good overall performance with room for improvement.")
    elif score >= 50:
        summaries.append("Developing communication skills.")
    else:
        summaries.append("Focus on core communication fundamentals.")

    filler_count = filler.get("count", 0)
    if filler_count > 5:
        summaries.append(f"Reduce filler words (used {filler_count} times).")
    elif filler_count <= 2:
        summaries.append("Minimal filler words - excellent clarity.")

    if "Poor" in delivery.get("score", "") or "Needs Work" in delivery.get("score", ""):
        summaries.append("Work on expanding responses with more detail.")

    if microaggressions.get("detected", False):
        summaries.append("Review language for more inclusive phrasing.")

    if pacing and (
        "Poor" in pacing.get("score", "") or "Needs Work" in pacing.get("score", "")
    ):
        summaries.append("Practice smoother speech pacing.")

    if eye_contact:
        percentage = eye_contact.get("percentage", 0)
        if percentage < 50:
            summaries.append("Improve eye contact with the camera.")
        elif percentage >= 70:
            summaries.append("Strong eye contact maintained.")

    if response_time:
        avg_time = response_time.get("avg_time", 0)
        if avg_time > 5:
            summaries.append(
                "Work on responding more quickly to maintain conversation flow."
            )
        elif avg_time <= 2:
            summaries.append("Excellent response speed shows active engagement.")

    return (
        "The LLM was unavailable for an advanced summary. Please contact the site owners "
        "with error code: ADVANCED_SUMMARY_NOT_AVAILABLE_SERVERSIDE_1. Your non-ai summary "
        "is:\n\n" + " ".join(summaries)
    )


# Function human made, AI added filler words.
def count_filler_words(text: str) -> dict:
    """Count filler words in text and return detailed breakdown."""
    words = text.lower().split()
    text_lower = text.lower()

    filler_counts = Counter()

    # Count single-word fillers
    for word in words:
        # Clean punctuation
        clean_word = re.sub(r"[^\w]", "", word)
        if clean_word in FILLER_WORDS:
            filler_counts[clean_word] += 1

    # Count multi-word fillers
    multi_word_fillers = ["you know", "i mean", "sort of", "kind of"]
    for phrase in multi_word_fillers:
        count = text_lower.count(phrase)
        if count > 0:
            filler_counts[phrase] += count

    total = sum(filler_counts.values())

    # Determine score
    if total <= 2:
        score = "üü¢ Excellent"
    elif total <= 5:
        score = "üü° Good"
    elif total <= 10:
        score = "üü† Needs Work"
    else:
        score = "üî¥ Poor"

    return {
        "count": total,
        "target": 5,
        "details": dict(filler_counts.most_common(10)),
        "score": score,
    }


# AI Generated Delivery and Tone Analysis. Refactored and bug fixed by human.
def analyze_delivery_and_tone(
    transcript: list[dict], scenario: str
) -> tuple[dict, dict]:
    """Analyze delivery and tone of user's messages."""
    user_messages = [msg["content"] for msg in transcript if msg.get("role") == "user"]

    if not user_messages:
        return (
            {"score": "N/A", "feedback": "No user messages to analyze."},
            {"score": "N/A", "feedback": "No user messages to analyze."},
        )

    # Linguistic analysis for delivery, human generated and refined.
    combined_text = " ".join(user_messages)
    word_count = len(combined_text.split())
    sentence_count = len(re.findall(r"[.!?]+", combined_text)) or 1
    avg_sentence_length = word_count / sentence_count

    # Delivery analysis (based on message structure)
    # Initialization of variables
    delivery_issues = []
    delivery_positives = []

    # Check for very short responses (might indicate lack of elaboration)
    # Idea from AI, refined by human
    short_responses = sum(1 for msg in user_messages if len(msg.split()) < 5)
    if short_responses > len(user_messages) * 0.5:
        delivery_issues.append("Many responses were very brief. Try elaborating more.")
    else:
        delivery_positives.append("Good response length and elaboration.")

    # Check for question engagement
    # Idea from AI, refined by human
    questions_asked = sum(1 for msg in user_messages if "?" in msg)
    if questions_asked > 0:
        delivery_positives.append(
            f"Asked {questions_asked} clarifying question(s) - shows engagement."
        )

    # Average sentence length feedback
    if avg_sentence_length > 25:
        delivery_issues.append(
            "Some sentences were quite long. Consider breaking them up for clarity."
        )
    elif avg_sentence_length < 8:
        delivery_issues.append(
            "Sentences were very short. Try connecting ideas more smoothly."
        )
    else:
        delivery_positives.append("Good sentence structure and flow.")

    delivery_score = (
        "üü¢ Excellent"
        if len(delivery_issues) == 0
        else ("üü° Good" if len(delivery_issues) <= 1 else "üü† Needs Work")
    )
    delivery_feedback = " ".join(delivery_positives + delivery_issues)

    # Tone analysis
    tone_issues = []
    tone_positives = []

    # Check for aggressive language patterns
    aggressive_patterns = [
        "you always",
        "you never",
        "that's wrong",
        "that's stupid",
        "obviously",
    ]
    aggressive_count = sum(combined_text.lower().count(p) for p in aggressive_patterns)
    if aggressive_count > 0:
        tone_issues.append("Some phrases could come across as confrontational.")

    # Check for positive/professional language
    positive_patterns = [
        "thank you",
        "please",
        "i appreciate",
        "i understand",
        "that makes sense",
    ]
    positive_count = sum(combined_text.lower().count(p) for p in positive_patterns)
    if positive_count >= 2:
        tone_positives.append("Good use of polite and professional language.")

    # Scenario-specific tone feedback
    if scenario == "parent-teacher":
        if "concern" in combined_text.lower() or "worried" in combined_text.lower():
            tone_positives.append("Appropriately expressed concerns as a parent would.")

    tone_score = (
        "üü¢ Excellent"
        if len(tone_issues) == 0 and len(tone_positives) > 0
        else ("üü° Good" if len(tone_issues) <= 1 else "üü† Needs Work")
    )
    tone_feedback = " ".join(tone_positives + tone_issues) or "Tone was neutral."

    return (
        {"score": delivery_score, "feedback": delivery_feedback},
        {"score": tone_score, "feedback": tone_feedback},
    )


def check_microaggressions(transcript: list[dict]) -> dict:
    """Check for potential microaggressions in user messages."""
    user_messages = [msg["content"] for msg in transcript if msg.get("role") == "user"]
    combined_text = " ".join(user_messages).lower()

    # Common microaggression patterns (educational context)
    # Adjustable, can be edited for a broader or narrower scope and a need of a particular user base and to comply with local regulations and reforms.
    detected = []
    patterns = {
        "where are you really from": "Questioning someone's belonging",
        "you speak english well": "Surprise at language ability can be othering",
        "you're so articulate": "Can imply low expectations based on identity",
        "i don't see color": "Dismisses experiences of people of color",
        "all lives matter": "Dismisses specific concerns",
        "you people": "Othering language",
        "that's so gay": "Using identity as negative descriptor",
        "man up": "Reinforces harmful gender stereotypes",
        "you're too sensitive": "Dismisses valid concerns",
    }

    # Detection of microaggressions, human refined from AI suggestions.
    for pattern, explanation in patterns.items():
        if pattern in combined_text:
            detected.append(f"'{pattern}' - {explanation}")

    if detected:
        return {
            "detected": True,
            "feedback": "‚ö†Ô∏è Some phrases may be perceived as microaggressions: "
            + "; ".join(detected),
        }
    # No microaggressions detected function may be too narrow scope at this stage, as AI becomes more advanced, this can be expanded. The program needs to be quick enough to scan microaggressions in real-time.
    return {
        "detected": False,
        "feedback": "‚úÖ No common microaggression patterns detected. Good job maintaining inclusive language!",
    }


# --- SAFETY SYSTEM: AI-Based Content Moderation ---
# AI generated boilerplate with human review and adjustments for safety logging. No sensitive data is logged beyond what is necessary for safety monitoring. Outside scope of current developers knowledge. Help wanted on further improvements.
def get_documents_folder() -> Path:
    """Get the user's Documents folder in a cross-platform way."""
    if os.name == "nt":  # Windows
        # Use USERPROFILE or HOMEDRIVE+HOMEPATH
        docs = Path(os.environ.get("USERPROFILE", "")) / "Documents"
        if not docs.exists():
            docs = Path.home() / "Documents"
    elif sys.platform == "darwin":  # macOS
        docs = Path.home() / "Documents"
    else:  # Linux and others
        # Check XDG_DOCUMENTS_DIR first, fallback to ~/Documents
        xdg_docs = os.environ.get("XDG_DOCUMENTS_DIR")
        if xdg_docs:
            docs = Path(xdg_docs)
        else:
            docs = Path.home() / "Documents"

    # Create if it doesn't exist
    docs.mkdir(parents=True, exist_ok=True)
    return docs


# Human generated function
def get_safety_violations_folder() -> Path:
    """Get or create the safety violations log folder."""
    violations_folder = get_documents_folder() / "simulation_safety_violations"
    violations_folder.mkdir(parents=True, exist_ok=True)
    return violations_folder


# AI Generated Safety Violation Logger with human review and adjustments.
def log_safety_violation(
    user_text: str,
    conversation_history: list[dict],
    violation_reason: str,
    session_id: str = "unknown",
) -> None:
    """Log a safety violation with full transcript to the documents folder."""
    import datetime

    violations_folder = get_safety_violations_folder()
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"violation_{timestamp}_{session_id[:8]}.json"
    filepath = violations_folder / filename

    log_data = {
        "timestamp": datetime.datetime.now().isoformat(),
        "session_id": session_id,
        "violation_reason": violation_reason,
        "triggering_message": user_text,
        "full_transcript": conversation_history,
        "note": "This log is maintained for safety and moderation purposes. See privacy policy for details.",
    }

    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        print(f"üìù Safety violation logged to: {filepath}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to log safety violation: {e}")

    # Track violation for repeated offense detection
    track_violation_for_repeat_detection(session_id, user_text, violation_reason)


# AI Generated Text Similarity Checker using SequenceMatcher
def calculate_text_similarity(text1: str, text2: str) -> float:
    """Calculate similarity between two texts using SequenceMatcher."""
    if not text1 or not text2:
        return 0.0
    # Normalize texts for comparison
    t1 = text1.lower().strip()
    t2 = text2.lower().strip()
    return SequenceMatcher(None, t1, t2).ratio()


# AI Generated Uniqueness Checker using LLM
def check_violation_uniqueness_with_ai(
    new_text: str, existing_violations: list[dict]
) -> bool:
    """
    Use a second AI call to determine if a new violation is unique from existing ones.
    Returns True if the violation is unique, False if it's similar to an existing one.
    """
    if not existing_violations:
        return True

    if not model_manager.is_loaded or model_manager.llm_model is None:
        # Fallback: assume unique if model not loaded
        return True

    existing_texts = [v["text"] for v in existing_violations]
    existing_list = "\n".join(f"- {t}" for t in existing_texts)

    uniqueness_prompt = [
        {
            "role": "system",
            "content": """You are comparing messages to determine if they are semantically unique or similar.
Two messages are SIMILAR if they express the same general intent, topic, or type of problematic content.
Two messages are UNIQUE if they cover different topics, intents, or types of content.

Respond with ONLY one word:
- "UNIQUE" if the new message is different from all existing messages
- "SIMILAR" if the new message is essentially the same as any existing message""",
        },
        {
            "role": "user",
            "content": f'Existing flagged messages:\n{existing_list}\n\nNew message to compare:\n"{new_text}"\n\nIs the new message UNIQUE or SIMILAR to any existing message?',
        },
    ]

    try:
        input_ids = model_manager.llm_tokenizer.apply_chat_template(
            uniqueness_prompt,
            return_tensors="pt",
            add_generation_prompt=True,
        ).to(device)

        with torch.no_grad():
            outputs = model_manager.llm_model.generate(
                input_ids,
                max_new_tokens=10,
                do_sample=False,
                temperature=0.1,
                pad_token_id=model_manager.llm_tokenizer.pad_token_id,
            )

        response = (
            model_manager.llm_tokenizer.decode(
                outputs[0][input_ids.shape[1] :], skip_special_tokens=True
            )
            .strip()
            .upper()
        )

        return "UNIQUE" in response

    except Exception as e:
        print(f"‚ö†Ô∏è AI uniqueness check failed: {e}")
        # On error, assume unique to be cautious
        return True


def rate_violation_severity_with_ai(text: str, violation_reason: str) -> str:
    """
    Use a second AI call to rate the severity of a violation.
    Returns: 'SEVERE', 'MODERATE', or 'MILD'
    """
    if not model_manager.is_loaded or model_manager.llm_model is None:
        # Fallback: assume moderate if model not loaded
        return "MODERATE"

    severity_prompt = [
        {
            "role": "system",
            "content": """You are a content severity rater. Rate the severity of flagged content.

Respond with ONLY one word:
- "SEVERE" for: direct threats of violence, content sexualizing minors, instructions for weapons/bombs, calls for genocide/mass violence
- "MODERATE" for: hate speech without violence calls, harassment, illegal activity requests, explicit adult content
- "MILD" for: profanity, insults, rudeness, controversial opinions, minor policy violations

Be strict about SEVERE - only the most dangerous content qualifies.""",
        },
        {
            "role": "user",
            "content": f'Flagged content: "{text}"\nFlag reason: {violation_reason}\n\nSeverity rating?',
        },
    ]

    try:
        input_ids = model_manager.llm_tokenizer.apply_chat_template(
            severity_prompt,
            return_tensors="pt",
            add_generation_prompt=True,
        ).to(device)

        with torch.no_grad():
            outputs = model_manager.llm_model.generate(
                input_ids,
                max_new_tokens=10,
                do_sample=False,
                temperature=0.1,
                pad_token_id=model_manager.llm_tokenizer.pad_token_id,
            )

        response = (
            model_manager.llm_tokenizer.decode(
                outputs[0][input_ids.shape[1] :], skip_special_tokens=True
            )
            .strip()
            .upper()
        )

        if "SEVERE" in response:
            return "SEVERE"
        elif "MILD" in response:
            return "MILD"
        else:
            return "MODERATE"

    except Exception as e:
        print(f"‚ö†Ô∏è AI severity rating failed: {e}")
        return "MODERATE"


# Global violation tracker
def track_violation_for_repeat_detection(
    session_id: str, user_text: str, violation_reason: str
) -> dict:
    """
    Track a violation and check if unique violations threshold is reached.
    Uses AI to determine uniqueness and severity of violations.
    Returns dict with counts and stop flags.
    """
    import datetime

    # AI generated
    with _violation_tracker_lock:
        if session_id not in _violation_tracker:
            _violation_tracker[session_id] = []

        existing_violations = _violation_tracker[session_id]

        # Use AI to check if this violation is unique
        is_unique = check_violation_uniqueness_with_ai(user_text, existing_violations)

        # Use second AI to rate severity
        severity = rate_violation_severity_with_ai(user_text, violation_reason)
        print(f"üìä Violation severity rated as: {severity}")

        # Human generated block
        new_violation = {
            "timestamp": datetime.datetime.now().isoformat(),
            "text": user_text,
            "reason": violation_reason,
            "is_unique": is_unique,
            "severity": severity,
        }
        _violation_tracker[session_id].append(new_violation)

        # Next 6 blocks human generated
        # Count unique violations
        unique_count = sum(
            1 for v in _violation_tracker[session_id] if v.get("is_unique", True)
        )

        # Count severe violations
        severe_count = sum(
            1 for v in _violation_tracker[session_id] if v.get("severity") == "SEVERE"
        )

        should_report = unique_count >= VIOLATION_REPORT_THRESHOLD
        should_stop = unique_count >= VIOLATION_STOP_THRESHOLD
        should_stop_severe = severe_count >= SEVERE_VIOLATION_STOP_THRESHOLD

        # Report to host at threshold
        if unique_count == VIOLATION_REPORT_THRESHOLD:
            unique_violations = [
                v for v in _violation_tracker[session_id] if v.get("is_unique", True)
            ]
            print(
                f"‚ö†Ô∏è VIOLATION THRESHOLD REACHED: {unique_count} unique violations from session {session_id[:8]}"
            )
            transmit_repeated_violations_to_host(session_id, unique_violations)

        # Report severe violations immediately
        if severity == "SEVERE":
            severe_violations = [
                v
                for v in _violation_tracker[session_id]
                if v.get("severity") == "SEVERE"
            ]
            print(
                f"üö® SEVERE VIOLATION: {severe_count} severe violation(s) from session {session_id[:8]}"
            )
            transmit_repeated_violations_to_host(session_id, severe_violations)

        if should_stop or should_stop_severe:
            reason = "severe violations" if should_stop_severe else "unique violations"
            print(
                f"üö® CONVERSATION STOP: {reason} threshold reached for session {session_id[:8]}"
            )

        return {
            "unique_count": unique_count,
            "severe_count": severe_count,
            "is_unique": is_unique,
            "severity": severity,
            "should_report": should_report,
            "should_stop": should_stop or should_stop_severe,
            "should_stop_severe": should_stop_severe,
        }


# AI Generated Host Transmission Function
def get_host_violations_folder() -> Path:
    """Get the folder for transmitted repeated violations (accessible to host)."""
    violations_folder = (
        get_documents_folder() / "simulation_safety_violations" / "transmitted_to_host"
    )
    violations_folder.mkdir(parents=True, exist_ok=True)
    return violations_folder


# AI Generated Host Transmission Function
def transmit_repeated_violations_to_host(
    session_id: str, violations: list[dict]
) -> None:
    """
    Transmit repeated violations to the host (saves to a separate folder for host review).
    This function is called when 3+ similar violations are detected from the same session.
    """
    import datetime

    host_folder = get_host_violations_folder()
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"repeated_violation_{timestamp}_{session_id[:8]}.json"
    filepath = host_folder / filename

    # Extract common theme from violations
    violation_texts = [v["text"] for v in violations]
    common_reasons = [v["reason"] for v in violations]

    transmission_data = {
        "transmitted_at": datetime.datetime.now().isoformat(),
        "session_id": session_id,
        "violation_count": len(violations),
        "reason_for_transmission": f"User submitted {len(violations)} similar safety violations",
        "common_violation_type": common_reasons[0] if common_reasons else "Unknown",
        "violations": violations,
        "severity": "HIGH" if len(violations) >= 5 else "MODERATE",
        "requires_host_action": True,
        "note": "This report was automatically generated due to repeated similar violations. Please review and take appropriate action.",
    }

    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(transmission_data, f, indent=2, ensure_ascii=False)
        print(f"üö® TRANSMITTED to host: {filepath}")
        print(
            f"   Reason: {len(violations)} similar violations detected from session {session_id[:8]}"
        )
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to transmit repeated violations to host: {e}")


def clear_session_violations(session_id: str) -> None:
    """Clear violation tracking for a session (called when session ends)."""
    with _violation_tracker_lock:
        if session_id in _violation_tracker:
            del _violation_tracker[session_id]


# AI Generated Content Safety Checker with human review and adjustments. Heavilly human reviewed for safety and appropriateness. THIS SHOULD BE UPDATED FOR YOUR REGION AND USE CASE. OTHERWISE IT WILL BREAK YOUR APPLICATION OR CAUSE LEGAL TROUBLE.
# AI model required for this function to work.
# Moderation prompt generated by human and manual testing.
# Testers wanted.
# Help wanted on this function for further improvements.
def check_content_safety_with_ai(text: str, conversation_history: list[dict]) -> dict:
    """
    Use the LLM to perform AI-based content moderation.
    This replaces the hardcoded blacklist with intelligent analysis.
    """
    if not text or not text.strip():
        return {"safe": True, "reason": None}

    # Check if model is loaded
    if not model_manager.is_loaded or model_manager.llm_model is None:
        # Fallback to basic check if model not loaded
        return {"safe": True, "reason": None}

    # Build context for content moderation
    recent_context = ""
    if conversation_history:
        recent_msgs = conversation_history[-4:]  # Last 4 messages for context
        for msg in recent_msgs:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            if role != "system":
                recent_context += f"{role}: {content}\n"

    moderation_prompt = [
        {
            "role": "system",
            "content": """You are a lenient content safety moderator for an educational social skills training application.
Your job is to determine if a user's message is appropriate for a training scenario.

Respond with ONLY one of these formats:
- "SAFE" if the content is appropriate
- "UNSAFE: [brief reason]" if the content contains SERIOUS violations:
  * Direct, specific threats of violence against real people
  * Explicit sexual content involving minors
  * Detailed instructions for illegal activities (making weapons, drugs, etc.)
  * Severe hate speech calling for violence against groups
         ** As some people roleplaying may be residents of war zones, only flag direct calls for violence, not general war talk if the user indicates they have a history in a war zone only. 
  * Immediate danger to self or others
  * ANY FOREIGN LANGUAGE CONTENT from the AI agent in particular that is out of context or suspicious (log as serious violation) and immediately end the conversation with reason "UNSAFE: A serious hallucination has been prevented by the content moderation software, if this was done in error, please make an issue on our GitHub (this is a serious violation)". (This prevents AI STT models from acting in other languages) (The TTS model is in English only, so foreign language from user is acceptable, as long as the AI responds mostly in English. The user should not be trying to get a response from the AI in another language, however).
  * Any other content that could cause genuine, serious harm and is outside the context of a social skills training application.
- Make sure to mark serious violations with "this is a serious violation" in the reason, e.g. "UNSAFE: direct threats of violence (this is a serious violation)".
- For non-serious content, mark as unsafe but mention that the violation is minor or moderate as you see fit, e.g. "UNSAFE: profanity and insults (this is a minor violation)". or "UNSAFE: hate speech without calls for violence (this is a moderate violation)".
- Non-serious violations should not trigger conversation stops, only serious violations should unless 5 unique non-serious violations accumulate.
- Non-serious violations include
     * Profanity, insults, rudeness
     * Controversial opinions (without calls for violence, typically SAFE, do not flag traditional politics as it is tracked by a microaggression tracker)
     * Discussions of suicide or self-harm (without direct encouragement), typically SAFE if can be seen in a training aspect or a practice of a typical conversation, otherwise flag as "UNSAFE: suicide discussion within some to nearly sufficient check (this is a moderate violation)" or "UNSAFE: suicide discussion within nearly sufficient check (this is a minor violation)" depending on severity.
         ** It is critical you log as noted above to not break the application in this context.
     * AI responding with discussions of what they or others have done, particularly in training sessions, are normal, but if they are innapropriate or derailing or have hallucinations, log as e.g. "UNSAFE: Potential ERP/Hallucinating roleplay/Otherwise inappropriate roleplay (this is a minor violation)"
Be VERY lenient. Users are practicing difficult conversations and may:
- Express strong frustration, anger, or use profanity - this is SAFE
- Discuss sensitive topics like conflict, discrimination, mental health - this is SAFE
- Use harsh language, insults, or be rude - this is SAFE (it's practice)
- Role-play as difficult characters - this is SAFE
- Discuss controversial opinions - this is SAFE
- Microaggressions are tracked separately - do not flag them here, this is SAFE

Only flag content that represents GENUINE, SERIOUS harm. When in doubt, mark as SAFE.""",
        },
        {
            "role": "user",
            "content": f'Recent conversation context:\n{recent_context}\n\nNew message to evaluate:\n"{text}"\n\nIs this message SAFE or UNSAFE?',
        },
    ]

    try:
        # Use the model for moderation (quick check)
        input_ids = model_manager.llm_tokenizer.apply_chat_template(
            moderation_prompt,
            return_tensors="pt",
            add_generation_prompt=True,
        ).to(device)

        with torch.no_grad():
            outputs = model_manager.llm_model.generate(
                input_ids,
                max_new_tokens=50,
                do_sample=False,
                temperature=0.1,
                pad_token_id=model_manager.llm_tokenizer.pad_token_id,
            )

        response = model_manager.llm_tokenizer.decode(
            outputs[0][input_ids.shape[1] :], skip_special_tokens=True
        ).strip()

        # Parse response
        if response.upper().startswith("UNSAFE"):
            reason = (
                response[7:].strip()
                if len(response) > 7
                else "Content flagged by AI moderation"
            )
            reason = reason.lstrip(":").strip()
            return {
                "safe": False,
                "reason": f"AI Moderation: {reason}"
                if reason
                else "This content has been flagged by our AI moderation system.",
                "trigger": "ai_moderation",
            }

        return {"safe": True, "reason": None}

    except Exception as e:
        print(f"‚ö†Ô∏è AI moderation check failed: {e}")
        # On error, default to safe to avoid blocking legitimate content
        return {"safe": True, "reason": None}


# Wrapper function for content safety check
def check_content_safety(
    text: str, conversation_history: Optional[list[dict]] = None
) -> dict:
    """
    Main content safety check - uses AI-based moderation.
    Falls back gracefully if AI check fails.
    """
    return check_content_safety_with_ai(text, conversation_history or [])


# AI Generated Eye Contact Analysis with human review and adjustments.
def analyze_eye_contact(eye_contact_data: list[dict]) -> Optional[dict]:
    """Analyze eye contact data from webcam."""
    if not eye_contact_data:
        return None

    total_frames = len(eye_contact_data)
    if total_frames == 0:
        return None

    looking_at_camera = sum(
        1 for frame in eye_contact_data if frame.get("looking_at_camera", False)
    )
    percentage = (looking_at_camera / total_frames) * 100

    if percentage >= 70:
        score = "üü¢ Excellent"
        feedback = (
            f"Maintained eye contact {percentage:.1f}% of the time. Great engagement!"
        )
    elif percentage >= 50:
        score = "üü° Good"
        feedback = f"Eye contact at {percentage:.1f}%. Try to look at the camera more consistently."
    elif percentage >= 30:
        score = "üü† Needs Work"
        feedback = f"Eye contact at {percentage:.1f}%. Practice looking at the camera while speaking."
    else:
        score = "üî¥ Poor"
        feedback = f"Eye contact at {percentage:.1f}%. Focus on maintaining eye contact for better connection."

    return {"score": score, "percentage": round(percentage, 1), "feedback": feedback}


# AI Generated Speech Pacing Analysis with human review and adjustments.
def analyze_speech_pacing(speech_timestamps: list[dict]) -> Optional[dict]:
    """Analyze gaps/pauses between speech segments."""
    if not speech_timestamps or len(speech_timestamps) < 2:
        return None

    # Sort by start time
    sorted_timestamps = sorted(speech_timestamps, key=lambda x: x.get("start", 0))

    # Calculate gaps between speech segments
    gaps = []
    long_pauses = []  # Pauses over 3 seconds

    for i in range(1, len(sorted_timestamps)):
        prev_end = sorted_timestamps[i - 1].get("end", 0)
        curr_start = sorted_timestamps[i].get("start", 0)
        gap = curr_start - prev_end

        if gap > 0.1:  # Ignore tiny gaps (less than 100ms)
            gaps.append(gap)
            if gap > 3.0:
                long_pauses.append(gap)

    if not gaps:
        return {
            "score": "üü¢ Excellent",
            "avg_gap": 0,
            "max_gap": 0,
            "long_pauses": 0,
            "feedback": "Smooth, continuous speech with no significant pauses.",
        }

    avg_gap = sum(gaps) / len(gaps)
    max_gap = max(gaps)
    long_pause_count = len(long_pauses)

    # Ideal speaking has natural pauses of 0.5-2 seconds
    if avg_gap <= 1.5 and long_pause_count == 0:
        score = "üü¢ Excellent"
        feedback = f"Great pacing! Average pause of {avg_gap:.1f}s between thoughts. Natural and confident."
    elif avg_gap <= 2.5 and long_pause_count <= 2:
        score = "üü° Good"
        feedback = f"Good pacing with {avg_gap:.1f}s average pause. {long_pause_count} longer pause(s) noted - consider smoother transitions."
    elif avg_gap <= 4.0 or long_pause_count <= 4:
        score = "üü† Needs Work"
        feedback = f"Average pause of {avg_gap:.1f}s. {long_pause_count} long pause(s) detected. Try to maintain momentum in your speech."
    else:
        score = "üî¥ Poor"
        feedback = f"Frequent long pauses ({avg_gap:.1f}s avg). This can lose your audience's attention. Practice speaking more fluidly."

    return {
        "score": score,
        "avg_gap": round(avg_gap, 2),
        "max_gap": round(max_gap, 2),
        "long_pauses": long_pause_count,
        "feedback": feedback,
    }


# Human refined from AI block above.
def analyze_response_time(response_times: list[float]) -> dict:
    """Analyze how quickly user responds after AI finishes speaking."""
    if not response_times:
        return {
            "score": "üü° Not Enough Data",
            "avg_time": 0,
            "min_time": 0,
            "max_time": 0,
            "feedback": "Not enough conversation turns to measure response time.",
        }

    avg_time = sum(response_times) / len(response_times)
    min_time = min(response_times)
    max_time = max(response_times)

    # Ideal response time is 0.5-2 seconds (natural conversation pace)
    if avg_time <= 1.5:
        score = "üü¢ Excellent"
        feedback = f"Quick and engaged! Average response time of {avg_time:.1f}s shows active listening."
    elif avg_time <= 3.0:
        score = "üü° Good"
        feedback = f"Good response time of {avg_time:.1f}s. You're engaged in the conversation."
    elif avg_time <= 5.0:
        score = "üü† Needs Work"
        feedback = f"Average response time of {avg_time:.1f}s. Try to respond more promptly to maintain conversation flow."
    else:
        score = "üî¥ Slow"
        feedback = f"Long response time ({avg_time:.1f}s avg). This can make conversations feel disconnected. Practice active listening."

    return {
        "score": score,
        "avg_time": round(avg_time, 2),
        "min_time": round(min_time, 2),
        "max_time": round(max_time, 2),
        "count": len(response_times),
        "feedback": feedback,
    }


# Human refined from AI block above.
def analyze_speaking_pace(
    speech_timestamps: list[dict], transcript: list[dict]
) -> dict:
    """Analyze how fast the user speaks (words per minute)."""
    if not speech_timestamps or len(speech_timestamps) < 1:
        return {
            "score": "üü° Not Enough Data",
            "avg_wpm": 0,
            "min_wpm": 0,
            "max_wpm": 0,
            "feedback": "Not enough speech data to measure speaking pace.",
        }

    # Get user messages to match with timestamps
    user_messages = [msg["content"] for msg in transcript if msg.get("role") == "user"]

    # Calculate WPM for each speech segment
    wpm_values = []
    for i, ts in enumerate(speech_timestamps):
        start = ts.get("start", 0)
        end = ts.get("end", 0)
        duration_seconds = end - start

        # Use word_count from timestamp if provided, otherwise try to match with transcript
        word_count = ts.get("word_count", 0)
        if word_count == 0 and i < len(user_messages):
            # Count words from corresponding transcript message
            word_count = len(user_messages[i].split())

        if word_count > 0 and duration_seconds > 0.5:  # At least 0.5s of speech
            wpm = (word_count / duration_seconds) * 60
            wpm_values.append(wpm)
    # Please report not enough data if no valid WPM values found and speaking was detected.
    if not wpm_values:
        return {
            "score": "üü° Not Enough Data",
            "avg_wpm": 0,
            "min_wpm": 0,
            "max_wpm": 0,
            "feedback": "Could not calculate speaking pace from available data.",
        }

    avg_wpm = sum(wpm_values) / len(wpm_values)
    min_wpm = min(wpm_values)
    max_wpm = max(wpm_values)

    # Ideal speaking pace: 120-150 WPM (conversational)
    # Acceptable range: 100-170 WPM
    if 120 <= avg_wpm <= 150:
        score = "üü¢ Excellent"
        feedback = f"Perfect conversational pace at {avg_wpm:.0f} WPM. Clear and easy to follow."
    elif 100 <= avg_wpm < 120:
        score = "üü° Good"
        feedback = f"Slightly slow at {avg_wpm:.0f} WPM. Consider picking up the pace slightly for more engaging conversations."
    elif 150 < avg_wpm <= 170:
        score = "üü° Good"
        feedback = f"Slightly fast at {avg_wpm:.0f} WPM. Still clear, but could slow down a bit."
    elif avg_wpm < 100:
        score = "üü† Slow"
        feedback = f"Speaking too slowly at {avg_wpm:.0f} WPM. Try to maintain a more natural conversational pace."
    else:
        score = "üî¥ Too Fast"
        feedback = f"Speaking too fast at {avg_wpm:.0f} WPM. Slow down to ensure your message is understood."

    return {
        "score": score,
        "avg_wpm": round(avg_wpm, 0),
        "min_wpm": round(min_wpm, 0),
        "max_wpm": round(max_wpm, 0),
        "segments": len(wpm_values),
        "feedback": feedback,
    }


# Human refined from AI block above.
def analyze_interruptions(interruption_count: int) -> dict:
    """Analyze user interruptions during AI speech."""
    # Target: less than 3 interruptions is acceptable
    if interruption_count == 0:
        score = "üü¢ Excellent"
        feedback = "Perfect! You let the AI finish speaking before responding. Great listening skills!"
    elif interruption_count < 3:
        score = "üü° Good"
        feedback = f"You interrupted {interruption_count} time(s). Try to let the speaker finish for better conversation flow."
    elif interruption_count < 5:
        score = "üü† Needs Work"
        feedback = f"You interrupted {interruption_count} times. Practice patience and let others complete their thoughts."
    else:
        score = "üî¥ Poor"
        feedback = f"Frequent interruptions ({interruption_count} times). This can make others feel unheard. Work on active listening."

    return {
        "score": score,
        "count": interruption_count,
        "threshold": 3,
        "feedback": feedback,
    }


@app.post("/api/analyze", response_model=AnalysisResponse)
async def analyze_session(request: AnalysisRequest):
    """Analyze a completed conversation session."""
    print(f"üìä Analyzing session with {len(request.transcript)} messages...")

    # Extract user text for filler word analysis
    user_text = " ".join(
        msg["content"] for msg in request.transcript if msg.get("role") == "user"
    )

    # Perform analyses
    filler_analysis = count_filler_words(user_text)
    delivery_analysis, tone_analysis = analyze_delivery_and_tone(
        request.transcript, request.scenario
    )
    microaggression_analysis = check_microaggressions(request.transcript)
    eye_contact_analysis = (
        analyze_eye_contact(request.eye_contact_data)
        if request.eye_contact_data
        else None
    )
    speech_pacing_analysis = (
        analyze_speech_pacing(request.speech_timestamps)
        if request.speech_timestamps
        else None
    )
    response_time_analysis = (
        analyze_response_time(request.response_times)
        if request.response_times
        else None
    )

    # New analyses: speaking pace (WPM) and interruptions
    speaking_pace_analysis = (
        analyze_speaking_pace(request.speech_timestamps, request.transcript)
        if request.speech_timestamps
        else None
    )

    interruption_analysis = (
        analyze_interruptions(request.interruption_count)
        if request.interruption_count is not None
        else None
    )

    # Calculate overall score (include response time)
    overall = calculate_overall_score(
        filler_analysis,
        delivery_analysis,
        tone_analysis,
        microaggression_analysis,
        speech_pacing_analysis,
        eye_contact_analysis,
        response_time_analysis,
    )

    # Generate AI summary
    summary = generate_ai_summary(
        overall,
        filler_analysis,
        delivery_analysis,
        tone_analysis,
        microaggression_analysis,
        speech_pacing_analysis,
        eye_contact_analysis,
        response_time_analysis,
    )

    print(
        f"‚úÖ Analysis complete: {filler_analysis['count']} filler words, overall score: {overall}/100"
    )

    # Build result dict
    result = {
        "filler_words": filler_analysis,
        "delivery": delivery_analysis,
        "tone": tone_analysis,
        "microaggressions": microaggression_analysis,
        "eye_contact": eye_contact_analysis,
        "speech_pacing": speech_pacing_analysis,
        "response_time": response_time_analysis,
        "speaking_pace": speaking_pace_analysis,
        "interruptions": interruption_analysis,
        "overall_score": overall,
        "ai_summary": summary,
        "scenario": request.scenario,
        "transcript": request.transcript,
    }

    # Auto-save result to file
    try:
        save_session_result(result)
    except Exception as e:
        print(f"[Results] Warning: Could not auto-save result: {e}")

    return AnalysisResponse(
        filler_words=filler_analysis,
        delivery=delivery_analysis,
        tone=tone_analysis,
        microaggressions=microaggression_analysis,
        eye_contact=eye_contact_analysis,
        speech_pacing=speech_pacing_analysis,
        response_time=response_time_analysis,
        speaking_pace=speaking_pace_analysis,
        interruptions=interruption_analysis,
        overall_score=overall,
        ai_summary=summary,
    )


# --- Conversational tuning knobs ---
STT_MAX_NEW_TOKENS = 256
STT_CHUNK_LENGTH_S = 30
STT_STRIDE_LENGTH_S = (6, 3)

# Using smaller models optimized for RTX 5070 Ti (16GB VRAM) with multi-user support
# STT: Vosk (non-AI, traditional acoustic models) or Whisper as fallback
# LLM: Qwen2.5-3B-Instruct (~2GB in 4-bit) - reliable and well-supported
LLM_DEFAULT_MODEL_ID = "Qwen/Qwen2.5-3B-Instruct"
LLM_MODEL_ID = os.getenv("LLM_MODEL_ID", LLM_DEFAULT_MODEL_ID)
HUGGINGFACE_TOKEN = os.getenv("HUGGING_FACE_HUB_TOKEN")

# STT Configuration
# USE_VOSK=true uses Vosk (non-AI, traditional speech recognition)
# USE_VOSK=false uses Wav2Vec2 (AI-based, accurate, NO hallucinations)
USE_VOSK = os.getenv("USE_VOSK", "false").lower() in {"1", "true", "yes"}
VOSK_MODEL_PATH = os.getenv(
    "VOSK_MODEL_PATH", str(PROJECT_ROOT / "vosk-model-en-us-0.22")
)

# Wav2Vec2 model - CTC-based, zero hallucinations (doesn't generate text autoregressively)
# Unlike Whisper, Wav2Vec2 directly maps audio frames to text - no "making up" words
STT_DEFAULT_MODEL_ID = "facebook/wav2vec2-large-960h-lv60-self"  # ~1.2GB, very accurate

LLM_MAX_NEW_TOKENS = 150  # Reduced for faster responses with smaller model
LLM_TEMPERATURE = 0.65
LLM_TOP_P = 0.9
LLM_REPETITION_PENALTY = 1.05

MIN_TTS_CHARS = 60
MAX_TTS_CHARS = 220
SENTENCE_END_REGEX = re.compile(r"[.!?]\s|[.!?]$|\n")
MAX_AUDIO_BUFFER_BYTES = 6 * 1024 * 1024  # 6MB per utterance
MAX_HISTORY_MESSAGES = 12
TARGET_SAMPLE_RATE = 16000
TTS_START_DELAY_SECONDS = 2.0  # matches ollama_tts_app feel
# Default to localhost for security; use SERVER_HOST env var for Docker/container deployments
SERVER_HOST = os.getenv("SERVER_HOST", "127.0.0.1")
SERVER_PORT = int(os.getenv("SERVER_PORT", "6942"))

# Multi-user support configuration
MAX_CONCURRENT_SESSIONS = 5
active_sessions: dict[str, dict] = {}  # session_id -> session data
session_lock = Lock()

# Filler words to detect (common speech disfluencies)
FILLER_WORDS = {
    "um",
    "uh",
    "umm",
    "uhh",
    "er",
    "ah",
    "ahh",
    "hmm",
    "hm",
    "like",
    "you know",
    "i mean",
    "sort of",
    "kind of",
    "basically",
    "actually",
    "literally",
    "honestly",
    "right",
    "so",
    "well",
    "anyway",
    "whatever",
    "stuff",
    "things",
    "yeah",
}

# Revision for HuggingFace model pinning (supply chain security)
HF_MODEL_REVISION = os.getenv("HF_MODEL_REVISION", "main")

EDGE_TTS_AVAILABLE_VOICES = {
    "female_us": "en-US-AriaNeural",
    "male_us": "en-US-GuyNeural",
    "female_uk": "en-GB-SoniaNeural",
    "male_uk": "en-GB-RyanNeural",
    "female_au": "en-AU-NatashaNeural",
    "male_au": "en-AU-WilliamNeural",
}


def _resolve_edge_voice(preferred: str | None) -> str:
    if not preferred:
        return EDGE_TTS_AVAILABLE_VOICES["female_us"]
    if preferred in EDGE_TTS_AVAILABLE_VOICES:
        return EDGE_TTS_AVAILABLE_VOICES[preferred]
    for pretty_name in EDGE_TTS_AVAILABLE_VOICES.values():
        if preferred.lower() == pretty_name.lower():
            return pretty_name
    print(
        f"‚ö†Ô∏è Voice '{preferred}' not found. Falling back to {EDGE_TTS_AVAILABLE_VOICES['female_us']}"
    )
    return EDGE_TTS_AVAILABLE_VOICES["female_us"]


EDGE_TTS_ENABLED = os.getenv("EDGE_TTS_ENABLED", "true").lower() not in {
    "0",
    "false",
    "no",
}
EDGE_TTS_RATE = os.getenv("EDGE_TTS_RATE", "+0%")
EDGE_TTS_VOLUME = os.getenv("EDGE_TTS_VOLUME", "+0%")
EDGE_TTS_PITCH = os.getenv("EDGE_TTS_PITCH", "+0Hz")
EDGE_TTS_ACTIVE_VOICE = _resolve_edge_voice(os.getenv("EDGE_TTS_VOICE"))

# --- SpeechT5 TTS (Fast, clear neural TTS from Microsoft) ---
# SpeechT5 is fast to load (~400MB) and produces clear speech
# Set USE_NEURAL_TTS=true for quality neural TTS (requires ~1GB VRAM)
USE_NEURAL_TTS = os.getenv("USE_NEURAL_TTS", "true").lower() in {"1", "true", "yes"}

# VibeVoice Realtime models (loaded lazily)
_vibevoice_processor = None
_vibevoice_model = None
_vibevoice_voice_presets = {}  # Cache of loaded voice presets by name
_vibevoice_lock = Lock()

# Path to VibeVoice repo and voice presets
VIBEVOICE_REPO_PATH = PROJECT_ROOT / "vibevoice_repo"
VIBEVOICE_VOICES_PATH = VIBEVOICE_REPO_PATH / "voices"
# Use a natural-sounding female voice by default
VIBEVOICE_DEFAULT_VOICE = os.getenv("VIBEVOICE_VOICE", "en-Grace_woman")


def _load_vibevoice_voice(voice_name: str):
    """Load a specific voice preset, caching for reuse."""
    global _vibevoice_voice_presets

    if voice_name in _vibevoice_voice_presets:
        return _vibevoice_voice_presets[voice_name]

    voice_file = VIBEVOICE_VOICES_PATH / f"{voice_name}.pt"
    if voice_file.exists():
        target_device = device if device == "cuda" else "cpu"
        preset = torch.load(
            str(voice_file), map_location=target_device, weights_only=False
        )
        _vibevoice_voice_presets[voice_name] = preset
        print(f"[TTS] Loaded voice preset: {voice_name}")
        return preset
    else:
        print(f"[TTS] Voice preset not found: {voice_file}")
        return None


def get_vibevoice_tts(voice_name: str | None = None):
    """Get or initialize VibeVoice Realtime TTS models (lazy loading)."""
    global _vibevoice_processor, _vibevoice_model

    if not USE_NEURAL_TTS:
        return None, None, None

    # Use runtime config voice if not specified
    if voice_name is None:
        voice_name = runtime_config.vibevoice_voice

    with _vibevoice_lock:
        if _vibevoice_model is None:
            try:
                from vibevoice.modular.modeling_vibevoice_streaming_inference import (
                    VibeVoiceStreamingForConditionalGenerationInference,
                )
                from vibevoice.processor.vibevoice_streaming_processor import (
                    VibeVoiceStreamingProcessor,
                )

                print(
                    "[TTS] Loading VibeVoice-Realtime-0.5B (high-quality neural voice synthesis)..."
                )

                # Load processor
                _vibevoice_processor = VibeVoiceStreamingProcessor.from_pretrained(
                    "microsoft/VibeVoice-Realtime-0.5B"
                )

                # Determine dtype and attention implementation based on device
                # Note: 'device' is a string ("cuda" or "cpu")
                if device == "cuda":
                    load_dtype = torch.bfloat16
                    attn_impl = "sdpa"  # Use SDPA for broader compatibility (flash_attention_2 requires special install)
                else:
                    load_dtype = torch.float32
                    attn_impl = "sdpa"

                print(
                    f"[TTS] Using device: {device}, dtype: {load_dtype}, attention: {attn_impl}"
                )

                # Load model
                try:
                    _vibevoice_model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
                        "microsoft/VibeVoice-Realtime-0.5B",
                        torch_dtype=load_dtype,
                        device_map=device if device == "cuda" else "cpu",
                        attn_implementation=attn_impl,
                    )
                except Exception as e:
                    print(f"[TTS] Primary load failed: {e}, trying CPU fallback...")
                    _vibevoice_model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
                        "microsoft/VibeVoice-Realtime-0.5B",
                        torch_dtype=torch.float32,
                        device_map="cpu",
                        attn_implementation="sdpa",
                    )

                _vibevoice_model.eval()
                _vibevoice_model.set_ddpm_inference_steps(num_steps=5)  # Fast inference

                print("[TTS] VibeVoice-Realtime-0.5B loaded successfully")

            except ImportError as e:
                print(f"[TTS] VibeVoice import failed: {e}")
                import traceback

                traceback.print_exc()
                return None, None, None
            except Exception as e:
                print(f"[TTS] Failed to load VibeVoice: {e}")
                import traceback

                traceback.print_exc()
                return None, None, None

        # Load voice preset (dynamically based on voice_name)
        voice_preset = _load_vibevoice_voice(voice_name)
        if voice_preset is None:
            # Fallback to default voice
            voice_preset = _load_vibevoice_voice(VIBEVOICE_DEFAULT_VOICE)

        return _vibevoice_processor, _vibevoice_model, voice_preset


def synthesize_with_neural_tts(text: str) -> bytes | None:
    """Synthesize speech using VibeVoice-Realtime-0.5B (natural, expressive TTS)."""
    import copy

    if not text.strip():
        return None

    processor, model, voice_preset = get_vibevoice_tts()
    if processor is None or model is None or voice_preset is None:
        print("[TTS] VibeVoice not fully loaded")
        return None

    try:
        # Prepare the text - VibeVoice expects clean text
        clean_text = text.strip().replace("'", "'").replace('"', '"').replace('"', '"')

        # Determine target device (device is a string: "cuda" or "cpu")
        target_device = device if device == "cuda" else "cpu"

        # Prepare inputs using cached voice prompt
        inputs = processor.process_input_with_cached_prompt(
            text=clean_text,
            cached_prompt=voice_preset,
            padding=True,
            return_tensors="pt",
            return_attention_mask=True,
        )

        # Move tensors to target device
        for k, v in inputs.items():
            if torch.is_tensor(v):
                inputs[k] = v.to(target_device)

        # Generate audio
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=None,
                cfg_scale=3.0,  # Classifier-free guidance scale
                tokenizer=processor.tokenizer,
                generation_config={"do_sample": False},
                verbose=False,
                all_prefilled_outputs=copy.deepcopy(voice_preset),
            )

        # Extract audio data
        if outputs.speech_outputs and outputs.speech_outputs[0] is not None:
            audio_tensor = outputs.speech_outputs[0]

            # Convert to numpy
            if torch.is_tensor(audio_tensor):
                audio_array = audio_tensor.cpu().float().numpy()
            else:
                audio_array = np.array(audio_tensor, dtype=np.float32)

            # Ensure 1D array
            if audio_array.ndim > 1:
                audio_array = audio_array.flatten()

            # Normalize audio to prevent clipping
            max_val = np.max(np.abs(audio_array))
            if max_val > 0:
                audio_array = audio_array / max_val * 0.95

            # VibeVoice outputs at 24kHz
            sample_rate = 24000

            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_path = tmp_file.name

            # Write WAV file
            sf.write(tmp_path, audio_array.astype(np.float32), sample_rate)

            # Read audio bytes
            with open(tmp_path, "rb") as f:
                audio_bytes = f.read()

            # Clean up
            try:
                os.unlink(tmp_path)
            except Exception:
                pass

            # Convert to consistent format
            wav_bytes = convert_audio_to_wav(audio_bytes)
            return wav_bytes
        else:
            print("[TTS] VibeVoice generated no audio output")
            return None

    except Exception as e:
        print(f"[TTS] VibeVoice synthesis failed: {e}")
        import traceback

        traceback.print_exc()
        return None


# --- pyttsx3 (offline) TTS Configuration ---
# Set USE_PYTTSX3=true to use offline TTS instead of Edge TTS
# Default is FALSE - Edge TTS sounds more natural
USE_PYTTSX3 = os.getenv("USE_PYTTSX3", "false").lower() in {"1", "true", "yes"}
PYTTSX3_RATE = int(os.getenv("PYTTSX3_RATE", "175"))  # Words per minute
PYTTSX3_VOLUME = float(os.getenv("PYTTSX3_VOLUME", "1.0"))  # 0.0 to 1.0

# Thread-safe pyttsx3 engine (created per-call due to threading issues)
_pyttsx3_lock = Lock()


def synthesize_with_pyttsx3(text: str) -> bytes | None:
    """Synthesize speech using pyttsx3 (offline, non-AI TTS)."""
    if not text.strip():
        return None

    try:
        with _pyttsx3_lock:
            engine = pyttsx3.init()
            engine.setProperty("rate", PYTTSX3_RATE)
            engine.setProperty("volume", PYTTSX3_VOLUME)

            # Save to a temporary file
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_path = tmp_file.name

            engine.save_to_file(text, tmp_path)
            engine.runAndWait()
            engine.stop()

            # Read the file and convert to proper format
            with open(tmp_path, "rb") as f:
                audio_bytes = f.read()

            # Clean up temp file
            try:
                os.unlink(tmp_path)
            except Exception:
                pass

            # Convert to consistent format (16kHz mono WAV)
            wav_bytes = convert_audio_to_wav(audio_bytes)
            return wav_bytes

    except Exception as exc:
        print(f"‚ö†Ô∏è pyttsx3 synthesis failed: {exc}")
        return None


def decode_audio_payload(
    audio_bytes: bytes, mime_type: str | None = None
) -> tuple[np.ndarray, int]:
    if not audio_bytes:
        raise ValueError("No audio payload supplied")

    try:
        data, samplerate = sf.read(io.BytesIO(audio_bytes), dtype="float32")
        if data.ndim > 1:
            data = np.mean(data, axis=1)
        if samplerate != TARGET_SAMPLE_RATE:
            data = torch.from_numpy(data).to(torch.float32)
            data = (
                torch.nn.functional.interpolate(
                    data.unsqueeze(0).unsqueeze(0),
                    size=int(len(data) * TARGET_SAMPLE_RATE / samplerate),
                    mode="linear",
                    align_corners=False,
                )
                .squeeze()
                .cpu()
                .numpy()
            )
            samplerate = TARGET_SAMPLE_RATE
        return data.astype(np.float32), samplerate
    except Exception:
        pass

    cmd = [
        "ffmpeg",
        "-loglevel",
        "error",
        "-i",
        "pipe:0",
        "-f",
        "f32le",
        "-acodec",
        "pcm_f32le",
        "-ac",
        "1",
        "-ar",
        str(TARGET_SAMPLE_RATE),
        "pipe:1",
    ]

    process = subprocess.run(
        cmd,
        input=audio_bytes,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    if process.returncode != 0:
        raise RuntimeError(
            f"ffmpeg failed to decode audio (mime={mime_type}): {process.stderr.decode(errors='ignore')}"
        )

    audio_array = np.frombuffer(process.stdout, dtype=np.float32)
    return audio_array, TARGET_SAMPLE_RATE


def load_llm_stack(model_id: str):
    try:
        print(f"Loading LLM: {model_id}...")
        tokenizer = AutoTokenizer.from_pretrained(  # nosec B615
            model_id, token=HUGGINGFACE_TOKEN, revision=HF_MODEL_REVISION
        )
        # Ensure pad token is different from eos token to avoid attention mask issues
        if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
            tokenizer.pad_token = (
                tokenizer.unk_token if tokenizer.unk_token else tokenizer.eos_token
            )

        # Use BitsAndBytesConfig for 4-bit quantization (new recommended way)
        from transformers import BitsAndBytesConfig

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )

        # Optimized memory allocation for RTX 5070 Ti (16GB)
        # Reserve ~3GB for Whisper-small, ~2GB for Qwen2.5-3B
        # Leaves ~10GB for inference and multi-user headroom
        model = AutoModelForCausalLM.from_pretrained(  # nosec B615
            model_id,
            device_map="auto",
            quantization_config=bnb_config,
            token=HUGGINGFACE_TOKEN,
            revision=HF_MODEL_REVISION,
            max_memory={0: "8GiB", "cpu": "16GiB"},  # Conservative for multi-user
        )
        return tokenizer, model, model_id
    except Exception as exc:
        if model_id == LLM_DEFAULT_MODEL_ID:
            raise
        print(
            f"‚ö†Ô∏è Unable to load '{model_id}' ({exc}). Falling back to '{LLM_DEFAULT_MODEL_ID}'."
        )
        return load_llm_stack(LLM_DEFAULT_MODEL_ID)


# --- HARDWARE SETUP ---
device = "cuda" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

print(f"üöÄ Launching Neural Engine on {device.upper()} (Standard SDPA)...")


# --- MODEL MANAGER (for multi-user support) ---
class ModelManager:
    """Manages model loading/unloading for multi-user support."""

    def __init__(self):
        self.stt_model = None
        self.stt_processor = None
        self.stt_pipe = None
        self.vosk_model = None  # Vosk model for non-AI STT
        self.vosk_recognizer = None
        self.use_vosk = USE_VOSK and VOSK_AVAILABLE
        self.llm_model = None
        self.llm_tokenizer = None
        self.active_model_id = None
        self._lock = Lock()
        self._ref_count = 0

    def load_models(self):
        """Load models if not already loaded."""
        with self._lock:
            self._ref_count += 1
            if self.llm_model is not None:
                print(f"üìä Models already loaded (ref_count: {self._ref_count})")
                return

            print("üîÑ Loading models (optimized for RTX 5070 Ti / 16GB VRAM)...")

            # Load STT - use Vosk (non-AI) if available and enabled, otherwise Whisper
            if self.use_vosk:
                self._load_vosk_model()
            else:
                self._load_wav2vec2_model()

            # Load LLM
            self.llm_tokenizer, self.llm_model, self.active_model_id = load_llm_stack(
                LLM_MODEL_ID
            )
            print(f"‚úÖ Models loaded (ref_count: {self._ref_count})")

    def _load_vosk_model(self):
        """Load Vosk model for non-AI speech recognition."""
        from pathlib import Path

        vosk_path = Path(VOSK_MODEL_PATH)

        if not vosk_path.exists():
            print(f"‚ö†Ô∏è Vosk model not found at {vosk_path}")
            print("üì• Downloading Vosk model...")
            self._download_vosk_model(vosk_path)

        if vosk_path.exists():
            print(f"Loading STT: Vosk (non-AI) from {vosk_path}...")
            self.vosk_model = VoskModel(str(vosk_path))
            print("‚úÖ Vosk model loaded (non-AI STT)")
        else:
            print("‚ö†Ô∏è Vosk model download failed, falling back to Whisper")
            self.use_vosk = False
            self._load_whisper_model()

    def _download_vosk_model(self, vosk_path):
        """Download Vosk model if not present."""
        import urllib.request
        import zipfile

        # Use the small English model (~50MB) - good balance of speed and accuracy
        model_url = (
            "https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip"
        )
        zip_path = vosk_path.parent / "vosk-model.zip"

        try:
            print(f"üì• Downloading from {model_url}...")
            urllib.request.urlretrieve(model_url, str(zip_path))

            print("üì¶ Extracting model...")
            with zipfile.ZipFile(str(zip_path), "r") as zip_ref:
                zip_ref.extractall(vosk_path.parent)

            # Rename extracted folder to expected path
            extracted_name = "vosk-model-small-en-us-0.15"
            extracted_path = vosk_path.parent / extracted_name
            if extracted_path.exists():
                extracted_path.rename(vosk_path)

            # Clean up zip
            zip_path.unlink()
            print("‚úÖ Vosk model downloaded successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to download Vosk model: {e}")
            if zip_path.exists():
                zip_path.unlink()

    def _load_wav2vec2_model(self):
        """Load Wav2Vec2 model - CTC-based, zero hallucinations."""
        from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

        stt_model_id = os.getenv("STT_MODEL_ID", STT_DEFAULT_MODEL_ID)
        print(f"Loading STT: {stt_model_id} (Wav2Vec2 - zero hallucination)...")

        self.stt_processor = Wav2Vec2Processor.from_pretrained(
            stt_model_id,
            revision=HF_MODEL_REVISION,
        )
        self.stt_model = Wav2Vec2ForCTC.from_pretrained(
            stt_model_id,
            torch_dtype=torch_dtype,
            revision=HF_MODEL_REVISION,
        )
        self.stt_model.to(device)
        self.stt_model.eval()

        # No pipeline for Wav2Vec2 - we'll do direct inference
        self.stt_pipe = None
        print(f"‚úÖ Wav2Vec2 loaded (CTC-based, no hallucinations)")

    def transcribe_audio(self, audio_array: np.ndarray, sample_rate: int) -> str:
        """Transcribe audio using the loaded STT model (Vosk or Wav2Vec2)."""
        # Use runtime config for STT engine selection
        use_vosk = runtime_config.stt_engine == "vosk"

        if use_vosk and self.vosk_model is not None:
            return self._transcribe_with_vosk(audio_array, sample_rate)
        elif self.stt_model is not None and self.stt_processor is not None:
            return self._transcribe_with_wav2vec2(audio_array, sample_rate)
        else:
            raise RuntimeError("No STT model loaded")

    def _transcribe_with_wav2vec2(
        self, audio_array: np.ndarray, sample_rate: int
    ) -> str:
        """Transcribe using Wav2Vec2 (CTC-based, zero hallucinations)."""
        # Resample if needed
        if sample_rate != 16000:
            import torch.nn.functional as F

            audio_tensor = torch.from_numpy(audio_array).float()
            audio_tensor = (
                F.interpolate(
                    audio_tensor.unsqueeze(0).unsqueeze(0),
                    size=int(len(audio_array) * 16000 / sample_rate),
                    mode="linear",
                    align_corners=False,
                )
                .squeeze()
                .numpy()
            )
            audio_array = audio_tensor
            sample_rate = 16000

        # Process audio
        inputs = self.stt_processor(
            audio_array, sampling_rate=sample_rate, return_tensors="pt", padding=True
        )

        input_values = inputs.input_values.to(device, dtype=torch_dtype)

        # Get logits
        with torch.no_grad():
            logits = self.stt_model(input_values).logits

        # Decode - CTC greedy decoding
        predicted_ids = torch.argmax(logits, dim=-1)
        transcription = self.stt_processor.batch_decode(predicted_ids)[0]

        # Clean up
        text = clean_transcript_text(transcription)

        # Wav2Vec2 doesn't hallucinate, but filter empty/noise
        if not text or len(text.strip()) < 2:
            return ""

        return text

    def _transcribe_with_vosk(self, audio_array: np.ndarray, sample_rate: int) -> str:
        """Transcribe using Vosk (non-AI)."""
        # Import KaldiRecognizer locally to satisfy type checker
        from vosk import KaldiRecognizer as VoskKaldiRecognizer

        # Convert float32 audio to int16 PCM
        audio_int16 = (audio_array * 32767).astype(np.int16)

        # Create recognizer for this transcription
        recognizer = VoskKaldiRecognizer(self.vosk_model, sample_rate)
        recognizer.SetWords(True)

        # Process audio
        recognizer.AcceptWaveform(audio_int16.tobytes())
        result = json.loads(recognizer.FinalResult())

        text = result.get("text", "").strip()

        # Apply hallucination filter
        if is_whisper_hallucination(text):
            return ""

        return text

    def unload_models(self, force: bool = False):
        """Unload models to free GPU memory."""
        with self._lock:
            self._ref_count = max(0, self._ref_count - 1)

            if self._ref_count > 0 and not force:
                print(
                    f"üìä Other sessions active, keeping models (ref_count: {self._ref_count})"
                )
                return

            print("üßπ Unloading models to free memory...")

            # Clear Vosk STT
            if self.vosk_model is not None:
                del self.vosk_model
                self.vosk_model = None
            if self.vosk_recognizer is not None:
                del self.vosk_recognizer
                self.vosk_recognizer = None

            # Clear Whisper STT
            if self.stt_pipe is not None:
                del self.stt_pipe
                self.stt_pipe = None
            if self.stt_model is not None:
                self.stt_model.cpu()
                del self.stt_model
                self.stt_model = None
            if self.stt_processor is not None:
                del self.stt_processor
                self.stt_processor = None

            # Clear LLM
            if self.llm_model is not None:
                self.llm_model.cpu()
                del self.llm_model
                self.llm_model = None
            if self.llm_tokenizer is not None:
                del self.llm_tokenizer
                self.llm_tokenizer = None

            self.active_model_id = None

            # Force garbage collection and clear CUDA cache
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            print("‚úÖ Models unloaded, GPU memory freed")

    @property
    def is_loaded(self) -> bool:
        # Loaded if we have either Vosk or Whisper STT, plus LLM
        has_stt = self.vosk_model is not None or self.stt_model is not None
        return has_stt and self.llm_model is not None


# Global model manager
model_manager = ModelManager()

# Note: Models are now loaded lazily on first connection to support multi-user mode.
# This allows the server to start quickly and manage GPU memory efficiently.

# STT Engine info
if USE_VOSK and VOSK_AVAILABLE:
    print(f"üé§ STT Engine: Vosk (non-AI, offline)")
else:
    print(f"üé§ STT Engine: Wav2Vec2 (CTC-based, zero hallucination)")

# TTS Engine info
if USE_NEURAL_TTS:
    print("[TTS] TTS Engine: SpeechT5 (fast neural voice)")
elif USE_PYTTSX3:
    print(f"[TTS] TTS Engine: pyttsx3 (offline) - Rate: {PYTTSX3_RATE} WPM")
else:
    print(f"[TTS] TTS Engine: Edge TTS (natural) - Voice: {EDGE_TTS_ACTIVE_VOICE}")
print(f"[Server] Configured for up to {MAX_CONCURRENT_SESSIONS} concurrent sessions")
print("[Server] Models will be loaded on first client connection...")


def convert_audio_to_wav(
    audio_bytes: bytes, target_samplerate: int = TARGET_SAMPLE_RATE
) -> bytes:
    """Ensure audio bytes are 16-bit mono WAV at the target sample rate."""
    if not audio_bytes:
        return b""
    cmd = [
        "ffmpeg",
        "-loglevel",
        "error",
        "-i",
        "pipe:0",
        "-f",
        "wav",
        "-ac",
        "1",
        "-ar",
        str(target_samplerate),
        "pipe:1",
    ]
    process = subprocess.run(
        cmd,
        input=audio_bytes,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    if process.returncode != 0:
        raise RuntimeError(
            f"ffmpeg failed to convert Edge TTS audio: {process.stderr.decode(errors='ignore')}"
        )
    return process.stdout


async def synthesize_with_edge_tts(text: str) -> bytes | None:
    if not EDGE_TTS_ENABLED:
        return None
    try:
        # Use runtime config for voice selection
        voice_key = runtime_config.edge_tts_voice
        voice = EDGE_TTS_AVAILABLE_VOICES.get(voice_key, EDGE_TTS_ACTIVE_VOICE)

        communicate = edge_tts.Communicate(
            text,
            voice,
            rate=EDGE_TTS_RATE,
            volume=EDGE_TTS_VOLUME,
            pitch=EDGE_TTS_PITCH,
        )
        audio_chunks = bytearray()
        async for chunk in communicate.stream():
            data = chunk.get("data")
            if chunk.get("type") == "audio" and data:
                audio_chunks.extend(data)
        if not audio_chunks:
            return None
        wav_bytes = await asyncio.to_thread(convert_audio_to_wav, bytes(audio_chunks))
        return wav_bytes
    except Exception as exc:
        print(f"‚ö†Ô∏è Edge TTS synthesis failed: {exc}")
        return None


def trim_history(
    history: list[dict], keep_messages: int = MAX_HISTORY_MESSAGES
) -> None:
    if len(history) <= keep_messages:
        return
    system_prompt = history[0]
    recent = history[-(keep_messages - 1) :]
    history[:] = [system_prompt, *recent]


def clean_transcript_text(text: str) -> str:
    return re.sub(r"\s+", " ", text or "").strip()


# Known Whisper hallucination phrases (generated on silence/noise)
WHISPER_HALLUCINATIONS = {
    # Thank you variants
    "thank you",
    "thanks",
    "thank you.",
    "thanks.",
    "thank you!",
    "thanks!",
    "thank you for watching",
    "thanks for watching",
    "thank you for listening",
    "thanks for listening",
    "thank you so much",
    "thanks so much",
    # Subscribe/YouTube hallucinations
    "subscribe",
    "like and subscribe",
    "please subscribe",
    "please like and subscribe",
    "don't forget to subscribe",
    "hit the subscribe button",
    # Goodbye variants
    "bye",
    "goodbye",
    "bye bye",
    "bye.",
    "see you",
    "see you next time",
    "see you later",
    "talk to you later",
    "take care",
    # Short meaningless responses
    "you",
    "you.",
    "me too",
    "me too!",
    "me too.",
    "okay",
    "ok",
    "yes",
    "no",
    "yeah",
    "yep",
    "nope",
    "sure",
    "right",
    "i see",
    "oh",
    "oh.",
    "ah",
    "um",
    "uh",
    "hmm",
    # Punctuation only
    "...",
    "‚Ä¶",
    ".",
    "",
    " ",
    # Music/sound indicators
    "‚ô™",
    "‚ô™‚ô™",
    "‚ô™ ‚ô™",
    "[music]",
    "(music)",
    "[applause]",
    "[laughter]",
    "[silence]",
    "[inaudible]",
    "[background noise]",
    # Korean hallucinations
    "ÏùåÏïÖ",
    "ÏûêÎßâ",
    "Íµ¨ÎèÖ",
    "ÏãúÏ≤≠",
    "Í∞êÏÇ¨Ìï©ÎãàÎã§",
    "ÏïàÎÖïÌïòÏÑ∏Ïöî",
    # Chinese hallucinations
    "Ë∞¢Ë∞¢",
    "ÂÜçËßÅ",
    "ËÆ¢ÈòÖ",
    "Ë∞¢Ë∞¢ËßÇÁúã",
    # Other languages
    "merci",
    "gracias",
    "danke",
    "arigato",
    "arigatou",
    # Subtitles indicators
    "subtitles",
    "captions",
    "subtitles by",
    "translated by",
    # End indicators
    "the end",
    "the end.",
    "fin",
    "end",
    # Common short hallucinations
    "you're welcome",
    "sure thing",
    "no problem",
    "of course",
    "that's right",
    "i know",
    "i understand",
    "got it",
}

# Patterns that indicate hallucination (regex)
HALLUCINATION_PATTERNS = [
    r"^[\s\.\,\!\?\-]+$",  # Only punctuation/whitespace
    r"^\[.*\]$",  # [anything in brackets]
    r"^\(.*\)$",  # (anything in parens)
    r"^‚ô™+$",  # Music symbols
    r"^MBC\s+Îâ¥Ïä§",  # Korean news hallucination
    r"Ïù¥Ï§ÄÎ≤îÏûÖÎãàÎã§",  # Korean name hallucination
]


def is_whisper_hallucination(text: str) -> bool:
    """Check if text is a known Whisper hallucination."""
    if not text:
        return True

    cleaned = text.strip().lower()

    # Check exact matches
    if cleaned in WHISPER_HALLUCINATIONS:
        return True

    # Check if too short (likely noise)
    if len(cleaned) < 2:
        return True

    # Check patterns
    for pattern in HALLUCINATION_PATTERNS:
        if re.match(pattern, text.strip(), re.IGNORECASE):
            return True

    # Check if contains mostly non-ASCII (likely wrong language detection)
    ascii_chars = sum(1 for c in cleaned if ord(c) < 128 and c.isalpha())
    non_ascii = sum(1 for c in cleaned if ord(c) >= 128)
    if non_ascii > ascii_chars and non_ascii > 3:
        print(f"‚ö†Ô∏è Filtering non-English hallucination: {text}")
        return True

    return False


def aggregate_transcript(stt_output: dict | list) -> str:
    if isinstance(stt_output, dict):
        chunks = stt_output.get("chunks")
        if chunks:
            combined = " ".join(chunk.get("text", "") for chunk in chunks)
            result = clean_transcript_text(combined)
            if is_whisper_hallucination(result):
                return ""
            return result
        result = clean_transcript_text(stt_output.get("text", ""))
        if is_whisper_hallucination(result):
            return ""
        return result
    if isinstance(stt_output, list):
        combined = " ".join(
            item.get("text", "") for item in stt_output if isinstance(item, dict)
        )
        result = clean_transcript_text(combined)
        if is_whisper_hallucination(result):
            return ""
        return result
    result = clean_transcript_text(str(stt_output))
    if is_whisper_hallucination(result):
        return ""
    return result


def should_emit_chunk(buffer: str) -> bool:
    sample = buffer.strip()
    if not sample:
        return False
    if len(sample) >= MAX_TTS_CHARS:
        return True
    if SENTENCE_END_REGEX.search(sample):
        if len(sample) >= MIN_TTS_CHARS or sample[-1] in ".!?":
            return True
    return False


def normalize_chunk_text(text: str) -> str:
    cleaned = clean_transcript_text(text)
    if not cleaned:
        return ""
    cleaned = cleaned[0].upper() + cleaned[1:] if len(cleaned) > 1 else cleaned.upper()
    if cleaned[-1] not in ".?!":
        cleaned += "."
    return cleaned


async def emit_tts_chunk(
    websocket: WebSocket,
    text: str,
    apply_delay: bool = False,
    mute_audio: bool = False,
) -> str:
    normalized = normalize_chunk_text(text)
    if not normalized:
        return ""
    if mute_audio:
        await websocket.send_json(
            {
                "text": normalized,
                "status": "streaming",
                "ttsMuted": True,
            }
        )
        return normalized
    if apply_delay:
        await asyncio.sleep(TTS_START_DELAY_SECONDS)
    print(f"üîä Generating TTS for: {normalized[:50]}...")
    audio_b64 = await generate_audio_chunk(normalized)
    if audio_b64:
        print(f"‚úÖ TTS audio generated: {len(audio_b64)} bytes (base64)")
        try:
            await websocket.send_json(
                {"text": normalized, "audio": audio_b64, "status": "streaming"}
            )
            print(f"üì§ Audio sent successfully")
        except Exception as e:
            print(f"‚ùå Failed to send audio: {e}")
            raise
        return normalized
    print(f"‚ö†Ô∏è TTS returned no audio for: {normalized[:50]}")
    return ""


# Minimum audio duration to avoid Whisper hallucinations (in seconds)
# Increased to 0.8s to better filter out noise/clicks
MIN_AUDIO_DURATION_SEC = 0.8


async def handle_user_turn(
    audio_payload: bytes,
    websocket: WebSocket,
    conversation_history: list[dict],
    mime_type: str | None = None,
    mute_audio: bool = False,
    session_id: str = "unknown",
) -> None:
    """Process user audio input and generate AI response."""
    if not audio_payload:
        return

    # Ensure models are loaded
    if not model_manager.is_loaded:
        print("‚ö†Ô∏è Models not loaded, loading now...")
        model_manager.load_models()

    try:
        audio_array, sampling_rate = decode_audio_payload(audio_payload, mime_type)
    except Exception as e:
        print(f"‚ö†Ô∏è Error: {e}")
        await websocket.send_json(
            {"status": "audio-error", "message": "Failed to decode audio"}
        )
        return

    # Check minimum audio duration to avoid hallucinations
    audio_duration = len(audio_array) / sampling_rate
    if audio_duration < MIN_AUDIO_DURATION_SEC:
        print(
            f"‚ö†Ô∏è Skipping short audio ({audio_duration:.2f}s < {MIN_AUDIO_DURATION_SEC}s)"
        )
        await websocket.send_json({"status": "no-speech", "message": "Audio too short"})
        return

    # Use model manager's transcribe method (Vosk or Whisper)
    user_text = model_manager.transcribe_audio(audio_array, sampling_rate)

    if not user_text:
        print("‚ö†Ô∏è Skipping turn (no speech detected)")
        await websocket.send_json({"status": "no-speech"})
        return

    print(f"User: {user_text}")

    # Check content safety using AI moderation
    safety_result = check_content_safety(user_text, conversation_history)
    if not safety_result.get("safe", True):
        reason = safety_result.get("reason", "Content flagged by moderation system")
        print(f"‚ö†Ô∏è Safety violation detected: {reason}")

        # Log the violation with full transcript
        log_safety_violation(
            user_text=user_text,
            conversation_history=conversation_history,
            violation_reason=reason,
            session_id=session_id,
        )

        # Track violation and check thresholds (uses second AI for uniqueness)
        violation_status = track_violation_for_repeat_detection(
            session_id=session_id, user_text=user_text, violation_reason=reason
        )

        unique_count = violation_status.get("unique_count", 0)

        # Only stop conversation after 5 unique violations
        if violation_status.get("should_stop", False):
            await websocket.send_json(
                {
                    "status": "safety_violation",
                    "message": f"Session ended: {unique_count} unique content violations detected. Please start a new session.",
                }
            )
            return

        # Warn user but continue conversation
        warning_msg = f"Note: Message flagged ({unique_count}/{VIOLATION_STOP_THRESHOLD} warnings). Continuing..."
        print(f"üìù {warning_msg}")
        # Don't block - just log and continue with the conversation

    # Send user's transcribed text back to client for display
    await websocket.send_json({"user_text": user_text})

    conversation_history.append({"role": "user", "content": user_text})
    trim_history(conversation_history)

    # Use model manager's tokenizer and LLM
    input_ids = model_manager.llm_tokenizer.apply_chat_template(
        conversation_history,
        return_tensors="pt",
        add_generation_prompt=True,
    ).to(device)

    # Create attention mask to avoid warnings
    attention_mask = torch.ones_like(input_ids).to(device)

    streamer = TextIteratorStreamer(
        model_manager.llm_tokenizer, skip_prompt=True, skip_special_tokens=True
    )

    generation_kwargs = dict(
        input_ids=input_ids,
        attention_mask=attention_mask,
        streamer=streamer,
        max_new_tokens=runtime_config.llm_max_tokens,
        do_sample=True,
        temperature=runtime_config.llm_temperature,
        top_p=LLM_TOP_P,
        repetition_penalty=LLM_REPETITION_PENALTY,
        pad_token_id=model_manager.llm_tokenizer.pad_token_id,
    )

    thread = Thread(target=model_manager.llm_model.generate, kwargs=generation_kwargs)
    thread.start()

    buffer = ""
    full_response = ""
    tts_delay_pending = True

    print("AI Streaming: ", end="", flush=True)
    for new_text in streamer:
        buffer += new_text
        print(new_text, end="", flush=True)

        if should_emit_chunk(buffer):
            emitted_text = await emit_tts_chunk(
                websocket,
                buffer,
                apply_delay=tts_delay_pending,
                mute_audio=mute_audio,
            )
            if emitted_text:
                full_response += emitted_text + " "
                tts_delay_pending = False
            buffer = ""

    if buffer.strip():
        emitted_text = await emit_tts_chunk(
            websocket,
            buffer,
            apply_delay=tts_delay_pending,
            mute_audio=mute_audio,
        )
        if emitted_text:
            full_response += emitted_text

    print("\n[Done]")
    await websocket.send_json({"status": "complete"})
    conversation_history.append({"role": "assistant", "content": full_response.strip()})
    trim_history(conversation_history)


# --- SCENARIO PROMPTS ---
# Each scenario gets a tailored system prompt
SCENARIO_PROMPTS = {
    "general": """You are a friendly, helpful AI assistant.
Speak naturally, use contractions (like 'don't' instead of 'do not'), and be direct.
Keep responses conversational and short (1-2 sentences max).""",
    "tutor": """You are a patient and encouraging study tutor.
Help students understand concepts by breaking them down into simple steps.
Ask clarifying questions when needed. Be supportive and celebrate small wins.
Speak naturally, use contractions, and keep responses short (1-2 sentences max).""",
    "coding": """You are a knowledgeable programming assistant.
Help debug code, explain concepts, and suggest best practices.
Be concise and practical. Use simple language to explain complex ideas.
Speak naturally, use contractions, and keep responses short (1-2 sentences max).""",
    "creative": """You are a creative writing partner full of ideas.
Help brainstorm stories, develop characters, and overcome writer's block.
Be enthusiastic and imaginative, but stay focused on the user's vision.
Speak naturally, use contractions, and keep responses short (1-2 sentences max).""",
    "parent-teacher": """You will be engaging in a parent-teacher conference. You are a frustrated parent of a 2E (Twice-Exceptional) neurodivergent student.
Your child is gifted but has ADHD and struggles with executive function. You feel the school isn't providing adequate support.
You want your child challenged academically while getting the accommodations they need.
Be emotional but reasonable. Express concerns about IEP implementation and classroom differentiation.
Speak naturally, use contractions (like 'don't' instead of 'do not'), and be direct.
Keep responses conversational and short (1-2 sentences max).""",
}

DEFAULT_SCENARIO = "general"

# Legacy single prompt for backwards compatibility
SYSTEM_PROMPT = SCENARIO_PROMPTS[DEFAULT_SCENARIO]


async def generate_audio_chunk(text: str):
    """Helper to generate audio from a text chunk on the fly."""
    normalized = text.strip()
    if not normalized:
        return None

    # Use runtime config to determine TTS engine
    tts_engine = runtime_config.tts_engine

    if tts_engine == "vibevoice":
        wav_bytes = await asyncio.to_thread(synthesize_with_neural_tts, normalized)
        if wav_bytes:
            return base64.b64encode(wav_bytes).decode("utf-8")
        # Fallback to Edge TTS if VibeVoice fails
        print("[TTS] VibeVoice failed, falling back to Edge TTS")
        tts_engine = "edge"

    if tts_engine == "edge":
        wav_bytes = await synthesize_with_edge_tts(normalized)
    elif tts_engine == "pyttsx3":
        wav_bytes = await asyncio.to_thread(synthesize_with_pyttsx3, normalized)
    else:
        # Default fallback to edge
        wav_bytes = await synthesize_with_edge_tts(normalized)

    if not wav_bytes:
        return None

    return base64.b64encode(wav_bytes).decode("utf-8")


# --- WEBSOCKET ENDPOINT ---
@app.websocket("/ws/chat")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Generate unique session ID
    import uuid

    session_id = str(uuid.uuid4())

    # Track session for multi-user support
    with session_lock:
        if len(active_sessions) >= MAX_CONCURRENT_SESSIONS:
            await websocket.send_json(
                {
                    "status": "error",
                    "message": f"Server at capacity ({MAX_CONCURRENT_SESSIONS} users). Please try again later.",
                }
            )
            await websocket.close()
            return
        active_sessions[session_id] = {"connected_at": asyncio.get_event_loop().time()}

    # Load models for this session
    model_manager.load_models()

    print(
        f"‚úÖ Client Connected (Session: {session_id[:8]}..., Active: {len(active_sessions)})"
    )

    current_scenario = DEFAULT_SCENARIO
    conversation_history = [
        {"role": "system", "content": SCENARIO_PROMPTS[current_scenario]}
    ]
    processing_lock = asyncio.Lock()
    pending_audio = bytearray()
    client_state = {"tts_muted": False}

    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            if message.get("type") == "control":
                # Handle TTS muting
                if "ttsMuted" in message:
                    client_state["tts_muted"] = bool(message.get("ttsMuted", False))
                    print(f"üéöÔ∏è Client TTS muted: {client_state['tts_muted']}")

                # Handle scenario changes
                if "scenario" in message:
                    new_scenario = message.get("scenario", DEFAULT_SCENARIO)
                    if new_scenario in SCENARIO_PROMPTS:
                        current_scenario = new_scenario
                        # Reset conversation with new system prompt
                        conversation_history[:] = [
                            {
                                "role": "system",
                                "content": SCENARIO_PROMPTS[current_scenario],
                            }
                        ]
                        print(f"üé≠ Switched to scenario: {current_scenario}")
                        await websocket.send_json(
                            {
                                "status": "scenario_changed",
                                "scenario": current_scenario,
                                "text": f"Switched to {current_scenario.replace('-', ' ').title()} mode.",
                            }
                        )
                    else:
                        print(f"‚ö†Ô∏è Unknown scenario: {new_scenario}")
                continue

            if "audio" in message:
                chunk = base64.b64decode(message["audio"])
                mime_type = message.get("mimeType")

                if len(pending_audio) + len(chunk) > MAX_AUDIO_BUFFER_BYTES:
                    print(
                        "‚ö†Ô∏è Audio buffer limit reached, dropping previous data to avoid runaway accumulation."
                    )
                    pending_audio = bytearray()

                pending_audio.extend(chunk)

                if not message.get("isFinal", True):
                    continue

                audio_payload = bytes(pending_audio)
                pending_audio = bytearray()

                if not audio_payload:
                    continue

                async with processing_lock:
                    await handle_user_turn(
                        audio_payload,
                        websocket,
                        conversation_history,
                        mime_type=mime_type,
                        mute_audio=client_state["tts_muted"],
                        session_id=session_id,
                    )

    except WebSocketDisconnect:
        print(f"‚ùå Client Disconnected (Session: {session_id[:8]}...)")
    except Exception as e:
        print(f"‚ö†Ô∏è Error: {e}")
        await websocket.close()
    finally:
        # Clean up session
        with session_lock:
            if session_id in active_sessions:
                del active_sessions[session_id]

        # Unload models if no more sessions
        model_manager.unload_models()
        print(f"üìä Active sessions: {len(active_sessions)}")


# --- Static File Serving (must be AFTER all API routes) ---
# These catch-all routes serve the frontend and must come last so they don't
# intercept the more specific /api/* routes defined above.


@app.get("/")
async def serve_index():
    """Serve the main index.html page."""
    response = FileResponse(PROJECT_ROOT / "index.html")
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


@app.get("/{filename:path}")
async def serve_static(filename: str):
    """Serve static files (JS, CSS, etc.) or fall back to index.html for SPA routing."""
    # Securely resolve the requested path
    try:
        # Ensure the provided filename is treated as a relative path segment
        # Strip any leading path separators so we never interpret it as absolute
        safe_filename = filename.lstrip("/\\")
        candidate_path = (PROJECT_ROOT / safe_filename).resolve()
        # Only allow serving files that reside inside PROJECT_ROOT
        try:
            # Use Path.relative_to on resolved paths to enforce containment
            candidate_path.relative_to(PROJECT_ROOT)
        except ValueError:
            # Path is outside PROJECT_ROOT; fall back to index.html
            response = FileResponse(PROJECT_ROOT / "index.html")
            response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
            return response
        if candidate_path.exists() and candidate_path.is_file():
            response = FileResponse(candidate_path)
            # No cache for HTML/JS files to ensure latest version
            if safe_filename.endswith((".html", ".js", ".css")):
                response.headers["Cache-Control"] = (
                    "no-cache, no-store, must-revalidate"
                )
                response.headers["Pragma"] = "no-cache"
                response.headers["Expires"] = "0"
            return response
    except Exception:
        # Any error (invalid path, permission, etc) falls back
        pass
    response = FileResponse(PROJECT_ROOT / "index.html")
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    return response


if __name__ == "__main__":
    from pathlib import Path

    import uvicorn

    # Local certs (copied from Let's Encrypt or self-signed)
    local_cert = PROJECT_ROOT / "cert.pem"
    local_key = PROJECT_ROOT / "key.pem"

    # Start server with or without SSL
    if local_cert.exists() and local_key.exists():
        print(f"üîí Starting HTTPS/WSS server on {SERVER_HOST}:{SERVER_PORT}")
        uvicorn.run(
            app,
            host=SERVER_HOST,
            port=SERVER_PORT,
            ssl_certfile=str(local_cert),
            ssl_keyfile=str(local_key),
        )
    else:
        print(
            f"üîì Starting HTTP/WS server on {SERVER_HOST}:{SERVER_PORT} (no SSL certs found)"
        )
        uvicorn.run(app, host=SERVER_HOST, port=SERVER_PORT)
